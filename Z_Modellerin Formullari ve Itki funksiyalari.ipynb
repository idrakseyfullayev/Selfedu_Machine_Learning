{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7f3509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6ab6655",
   "metadata": {},
   "source": [
    "## Supervised Learning  modellərinin Formulları və İtki (loss) funksiyaları"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec294ca",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Linear Regression**:\n",
    "   $$\n",
    "   \\text{Predict}: \\quad y = kx + b\n",
    "   $$\n",
    "   (Sadə xətti əlaqə, burada $ k $ - əmsal, $ b $ - bias).\n",
    "\n",
    "   $$\n",
    "   \\text{Itki Funksiyası (Loss)}: \\quad L = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "   $$\n",
    "   (MSE - Mean Squared Error, burada $ y_i $ gerçək dəyər, $ \\hat{y}_i $ isə modelin proqnozudur).\n",
    "\n",
    "---\n",
    "\n",
    "2. **SVM (Support Vector Machine)**:\n",
    "   $$\n",
    "   \\text{Predict}: \\quad y = \\text{sign}(w^T x + b)\n",
    "   $$\n",
    "   (Sinifləndirmə üçün, $ w $ - ağırlıqlar, $ x $ - girişlər, $ b $ - bias, $ \\text{sign} $ - aktivasiya funksiyası).\n",
    "\n",
    "   $$\n",
    "   \\text{Itki Funksiyası (Loss)}: \\quad L = \\sum_{i=1}^{N} \\max(0, 1 - y_i(w^T x_i + b))\n",
    "   $$\n",
    "   (Hinge Loss funksiyası, burada $ y_i $ - doğru sinif, $ w $ - ağırlıqlar, $ x_i $ - girişlər).\n",
    "\n",
    "---\n",
    "\n",
    "3. **Logistic Regression**:\n",
    "   $$\n",
    "   \\text{Predict}: \\quad y = \\frac{1}{1 + e^{-(wx + b)}}\n",
    "   $$\n",
    "   (Logistikanın sigmoidal funksiyası ilə ehtimal hesablanır, burada $ w $ - ağırlıqlar, $ x $ - girişlər, $ b $ - bias).\n",
    "\n",
    "   $$\n",
    "   \\text{Itki Funksiyası (Loss)}: \\quad L = - \\sum_{i=1}^{N} \\left( y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right)\n",
    "   $$\n",
    "   (Binary Cross-Entropy Loss funksiyası, burada $ \\hat{y}_i $ - modelin ehtimal proqnozu).\n",
    "\n",
    "---\n",
    "\n",
    "4. **Naive Bayes**:\n",
    "   $$\n",
    "   \\text{Predict}: \\quad P(C|X) = \\frac{P(X|C)P(C)}{P(X)}\n",
    "   $$\n",
    "   (Şartlı ehtimal, burada $ C $ sinif, $ X $ - xüsusiyyətlərdir).\n",
    "\n",
    "   $$\n",
    "   \\text{Itki Funksiyası (Loss)}: \\quad L = - \\sum_{i=1}^{N} \\log P(y_i | x_i)\n",
    "   $$\n",
    "   (Klassiklik ehtimalı əsasında verilənləri təsdiq etmək üçün istifadə olunur, burada $ y_i $ və $ x_i $ müvafiq sinif və girişdir).\n",
    "\n",
    "---\n",
    "\n",
    "5. **Decision Tree**:\n",
    "   (Yalnız qərarları və şaxələri izləyir, yəni girişlərin müxtəlif şərtlərə görə bölünməsi əsasında sinif təyin edilir. Dəqiq riyazi bir ifadə yoxdur, amma **predict** girişin hansı şaxəyə düşməsinə əsaslanır).\n",
    "\n",
    "   $$\n",
    "   \\text{Itki Funksiyası (Loss)}: \\quad L = \\sum_{i=1}^{N} \\left( \\text{Gini Impurity} \\text{ or } \\text{Entropy} \\right)\n",
    "   $$\n",
    "   (Çox vaxt **Gini Impurity** və ya **Entropy** istifadə edilir, burada **Entropy**: \n",
    "   $$\n",
    "   -\\sum_{i=1}^{M} p_i \\log(p_i)\n",
    "   $$\n",
    "   və **Gini Impurity**:\n",
    "   $$\n",
    "   1 - \\sum_{i=1}^{M} p_i^2\n",
    "   $$\n",
    "   burada $ p_i $ sinif ehtimallarıdır).\n",
    "\n",
    "---\n",
    "\n",
    "6. **Random Forest**:\n",
    "   $$\n",
    "   \\text{Predict}: \\quad y = \\frac{1}{N} \\sum_{i=1}^{N} \\hat{y}_i\n",
    "   $$\n",
    "   (Bir çox qərar ağacının nəticələrinin ortalaması alınır, burada $ \\hat{y}_i $ hər bir ağacın verdiyi nəticədir).\n",
    "\n",
    "   $$\n",
    "   \\text{Itki Funksiyası (Loss)}: \\quad L = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "   $$\n",
    "   (MSE - Mean Squared Error, burada $ y_i $ gerçək dəyər, $ \\hat{y}_i $ isə modelin proqnozudur).\n",
    "\n",
    "---\n",
    "\n",
    "Hər bir modelin **predict** funksiyası, modelin nəticə təyin etməsi üçün istifadə olunan əsas yanaşmanı göstərir, həmçinin **itki funksiyaları** hər bir modelin optimallaşdırılmasına kömək edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becedc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b39a552b",
   "metadata": {},
   "source": [
    "## **Boosting** modellərinin Formulları və İtki (loss) funksiyaları"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e99c151",
   "metadata": {},
   "source": [
    "Əlbəttə, **Boosting** metodlarının əsas modellərini və onların **itki (loss) funksiyalarını** aşağıda təqdim edirəm:\n",
    "\n",
    "---\n",
    "\n",
    "1. **AdaBoost (Adaptive Boosting)**:\n",
    "   $$\n",
    "   y = \\text{sign}\\left( \\sum_{i=1}^{M} \\alpha_i h_i(x) \\right)\n",
    "   $$\n",
    "   (Bir çox zəif modelin (adətən qərar ağacları) ağırlıqlı cəmini götürür. Hər modelin səhvini kompensasiya etmək üçün ağırlıqlar tənzimlənir).\n",
    "\n",
    "   $$\n",
    "   \\text{Itki Funksiyası (Loss)}: \\quad L = \\sum_{i=1}^{N} \\exp\\left( -y_i \\sum_{i=1}^{M} \\alpha_i h_i(x) \\right)\n",
    "   $$\n",
    "   (Exponential Loss - Hər iterasiyada modelin səhvini minimuma endirmək məqsədilə səhv siniflərin cəmini azaltmaq üçün istifadə olunur).\n",
    "\n",
    "---\n",
    "\n",
    "2. **Gradient Boosting**:\n",
    "   $$\n",
    "   y = \\sum_{i=1}^{M} \\alpha_i f_i(x)\n",
    "   $$\n",
    "   (Hər addımda model, əvvəlki modelin səhvlərini düzəltmək üçün optimallaşdırılır. $ f_i(x) $ hər bir ağacın çıxışı, $ \\alpha_i $ isə ağacın çəkisidir).\n",
    "\n",
    "   $$\n",
    "   \\text{Itki Funksiyası (Loss)}: \\quad L = \\sum_{i=1}^{N} \\left( y_i - f(x_i) \\right)^2\n",
    "   $$\n",
    "   (Mean Squared Error (MSE) - Gradient Boosting ümumiyyətlə səhvləri kvadratla minumumlaşdırır).\n",
    "\n",
    "---\n",
    "\n",
    "3. **XGBoost (Extreme Gradient Boosting)**:\n",
    "   $$\n",
    "   y = \\sum_{i=1}^{M} \\left( f_i(x) + \\lambda \\| f_i \\|^2 \\right)\n",
    "   $$\n",
    "   (Gradient Boosting-in optimallaşdırılmış versiyasıdır. $ \\lambda $ - L2 cərimə termi, əlavə olaraq, tənzimlənmiş cərimə və erkən dayandırma ilə optimallaşdırılır).\n",
    "\n",
    "   $$\n",
    "   \\text{Itki Funksiyası (Loss)}: \\quad L = \\sum_{i=1}^{N} \\left( y_i - f(x_i) \\right)^2 + \\lambda \\| f_i \\|^2\n",
    "   $$\n",
    "   (XGBoost həmçinin **regularization** (tənzimləmə) tətbiq edir, bu da modelin overfitting-i qarşısını alır).\n",
    "\n",
    "---\n",
    "\n",
    "4. **LightGBM (Light Gradient Boosting Machine)**:\n",
    "   $$\n",
    "   y = \\sum_{i=1}^{M} f_i(x)\n",
    "   $$\n",
    "   (XGBoost-a bənzərdir, lakin daha sürətli işləmək üçün xüsusi olaraq hazırlanmışdır. Leaf-wise axtarış və histogram əsaslı texnologiyalar istifadə edir).\n",
    "\n",
    "   $$\n",
    "   \\text{Itki Funksiyası (Loss)}: \\quad L = \\sum_{i=1}^{N} \\left( y_i - f(x_i) \\right)^2\n",
    "   $$\n",
    "   (LightGBM-də də MSE və **Cross-Entropy Loss** kimi ümumi itki funksiyaları istifadə olunur).\n",
    "\n",
    "---\n",
    "\n",
    "5. **CatBoost (Categorical Boosting)**:\n",
    "   $$\n",
    "   y = \\sum_{i=1}^{M} \\alpha_i f_i(x)\n",
    "   $$\n",
    "   (CatBoost, xüsusilə kateqorial dəyişənləri işləmək üçün optimallaşdırılmış gradient boosting metodudur və böyük məlumat dəstləri ilə yaxşı işləyir).\n",
    "\n",
    "   $$\n",
    "   \\text{Itki Funksiyası (Loss)}: \\quad L = \\sum_{i=1}^{N} \\left( y_i - f(x_i) \\right)^2\n",
    "   $$\n",
    "   (CatBoost da digər boosting modelləri kimi MSE və ya **Cross-Entropy Loss** istifadə edir).\n",
    "\n",
    "---\n",
    "\n",
    "### Ümumi Qayda:\n",
    "Boosting metodları ümumiyyətlə zəif modellərin ardıcıl tətbiq edilməsi ilə çalışır. Hər bir model, əvvəlki modelin səhvlərini düzəltməyə çalışır. Modellər sırasıyla düzəldilən səhvlərin cəmini təklif edir və nəticədə daha güclü bir model əldə edilir.\n",
    "\n",
    "Boosting metodları müxtəlif yanaşmalarla optimallaşdırılır və hər biri öz xüsusiyyətlərinə görə fərqlənir, amma ümumi məqsəd hər zaman səhvlərin azaldılması və proqnozun yaxşılaşdırılmasıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e65c6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "259bfdd7",
   "metadata": {},
   "source": [
    "## **Boosting** metodlarında $ f_i(x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaa214d",
   "metadata": {},
   "source": [
    "**Boosting** metodlarında $ f_i(x) $, hər bir zəif modelin verdiyi çıxışı göstərir, amma bu çıxış **sadə xətti bir funksiya** (yəni $ y = kx + b $) ilə məhdudlaşmır. Əksər hallarda, **Boosting** metodlarında istifadə edilən zəif modellər, məsələn, **qərar ağacları** və ya digər ağac əsaslı modellər olduğu üçün $ f_i(x) $ genelliklə daha mürəkkəb bir struktura sahib olur.\n",
    "\n",
    "Buna görə, **Boosting** modellərində $ f_i(x) $ bir **qərar ağacı** kimi daha mürəkkəb bir funksiya ola bilər. Qərar ağacları $ x $ dəyişənlərinə əsaslanaraq müəyyən şərtlərə görə qərar verir. \n",
    "\n",
    "**Qərar ağacları** modelində $ f_i(x) $ belə bir şeyə bənzəyə bilər:\n",
    "\n",
    "$$\n",
    "f_i(x) = \\begin{cases} \n",
    "      \\text{class}_1 & \\text{if } x \\text{ satisfies condition 1} \\\\\n",
    "      \\text{class}_2 & \\text{if } x \\text{ satisfies condition 2} \\\\\n",
    "      \\dots & \\dots\n",
    "   \\end{cases}\n",
    "$$\n",
    "\n",
    "Bu şərtlər, məsələn, $ x_1 $ və $ x_2 $ kimi xüsusiyyətlərə əsaslanaraq, müxtəlif siniflərə və ya dəyərlərə bölünməyə imkan verir.\n",
    "\n",
    "Başqa sözlə, **Boosting** metodlarında $ f_i(x) $, hər bir zəif modelin, məsələn, **qərar ağacının** verdiyi qiymətdir və bu qiymətlər əvvəlki modellərin səhvlərini düzəltmək üçün istifadə edilir.\n",
    "\n",
    "---\n",
    "\n",
    "**Sadə xətti modelin (yəni $ y = kx + b $) tətbiqi** yalnız çox sadə zəif modellər üçün istifadə edilə bilər. Lakin əksər zaman **Boosting** metodlarında daha mürəkkəb modellər (xüsusən **qərar ağacları**) istifadə olunur. \n",
    "\n",
    "Məsələn:\n",
    "- **AdaBoost** və **Gradient Boosting** çox vaxt qərar ağacları kimi zəif modellərlə işləyir.\n",
    "- **XGBoost**, **LightGBM** və **CatBoost** daha optimallaşdırılmış versiyalardır, amma onların da əsasında qərar ağacları və digər ağac əsaslı modellər dayanır.\n",
    "\n",
    "Bu səbəbdən **Boosting** metodlarında $ f_i(x) $, çox vaxt daha mürəkkəb strukturların nəticəsidir və yalnız xətti proqnozlarla məhdudlaşmır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9974cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
