{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4558ae65",
   "metadata": {},
   "source": [
    "## Gradient Descent istifadə edən modellər"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0744d89e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Xeyr, bütün bu modellər **Gradient Descent (GD)** istifadə etmir. Gəlin ayrı-ayrılıqda baxaq:  \n",
    "\n",
    "### **Gradient Descent istifadə edən modellər:**\n",
    "1. **Linear Regression** – Bəli, böyük datasetlərdə GD tez-tez istifadə olunur. Amma kiçik datasetlərdə normal tənliklərlə ($Normal\\ Equation$) həll edilə bilər.  \n",
    "2. **Logistic Regression** – Bəli, ehtimal funksiyasını maksimum etmək üçün GD istifadə olunur.  \n",
    "3. **Support Vector Machine (SVM)** – Bəli, xüsusən də `$SGDClassifier$` (Stochastic Gradient Descent) istifadə ediləndə GD tətbiq olunur. Amma ənənəvi SVM-lər **Quadratic Programming** üsulu ilə həll olunur.\n",
    "\n",
    "### **Gradient Descent istifadə etməyən modellər:**\n",
    "4. **Naïve Bayes** – Xeyr, çünki Bayes teoreminə əsaslanır və qapalı formada (analitik həll) hesablanır.  \n",
    "5. **Decision Tree (Design Tree)** – Xeyr, çünki ağac qurulması iterativ bölmələrə (splitting) əsaslanır, GD istifadə etmir.  \n",
    "6. **Random Forest** – Xeyr, çünki bir neçə Decision Tree modelinin birləşməsindən ibarətdir.  \n",
    "7. **Bayesian Methods** (Bayesian Logistic Regression və s.) – Əsasən Bayes qaydalarına və ehtimal paylanmalarına əsaslanır, GD istifadə etmir. Amma bəzən optimallaşdırma üçün variational inference üsullarında GD tətbiq oluna bilər.\n",
    "\n",
    "**Qısa Nəticə:**  \n",
    "- **Gradient Descent əsasən optimallaşdırma tələb edən modellərdə istifadə olunur.**  \n",
    "- **Tree-based (Decision Tree, Random Forest) və Naïve Bayes modellərində isə Gradient Descent tələb olunmur.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90670f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fa3677c",
   "metadata": {},
   "source": [
    "## Burada Gradient Descent harada işləyir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21879d7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Bəli, `LogisticRegression` modeli **Gradient Descent (GD) və digər optimallaşdırma metodlarından** istifadə edə bilər. Scikit-learn kitabxanasında `LogisticRegression` modelinin həlli üçün müxtəlif optimallaşdırma alqoritmləri mövcuddur.  \n",
    "\n",
    "### **Gradient Descent hansı hallarda istifadə olunur?**\n",
    "`LogisticRegression` modelində **solver (həll metodu) seçiminə görə** GD istifadə edilə və ya edilməyə bilər.  \n",
    "- **`solver='liblinear'`** – **GD istifadə etmir**, çünki bu, kiçik datasetlər üçün $Coordinate\\ Descent$ metodundan istifadə edir.  \n",
    "- **`solver='newton-cg'`, `solver='lbfgs'`, `solver='sag'`, `solver='saga'`** – Bunlar iterativ optimallaşdırma üsullarıdır, **specially `sag` və `saga` Stochastic Gradient Descent (SGD) istifadə edir.**  \n",
    "\n",
    "### **Kod nümunəsi – `saga` solver ilə Gradient Descent istifadə edək**  \n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dummy dataset yaradaq\n",
    "X, y = make_classification(n_samples=1000, n_features=5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression modelini SGD əsaslı həll edək\n",
    "model = LogisticRegression(solver='saga', max_iter=500, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Modelin təxminlərini çap edək\n",
    "print(\"Model Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "```\n",
    "\n",
    "### **Burada Gradient Descent harada işləyir?**\n",
    "- `solver='saga'` → **Stochastic Average Gradient Descent (SAGA) metodunu istifadə edir**.  \n",
    "- `max_iter=500` → **GD iterasiyalarının maksimum sayını təyin edir**.  \n",
    "- `fit(X_train, y_train)` → **Burada iterativ olaraq Gradient Descent işləyir və modelin parametrləri yenilənir**.\n",
    "\n",
    "Əgər daha kiçik datasetlərlə işləyirsinizsə, `liblinear` solverini istifadə etmək daha məqsədəuyğundur. Amma böyük datasetlərdə **GD əsaslı `saga` və `sag`** solverləri daha effektiv işləyir.\n",
    "\n",
    "**Nəticə:**  \n",
    "Bəli, **Logistic Regression modelində `sag` və `saga` solverləri Gradient Descent istifadə edir**. Ancaq default solver `lbfgs` olduğuna görə, bəzi hallarda GD əvəzinə ikinci dərəcəli metodlar ($quasi\\text{-}Newton$ metodları) istifadə olunur.\n",
    "\n",
    "--- \n",
    "\n",
    "Əlavə istəklərin olsa, həmişə burdayam!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aeb5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c6914ae",
   "metadata": {},
   "source": [
    "## Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52510edf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**`solver`** – `LogisticRegression` modelində **optimallaşdırma alqoritmini (itərativ minimizasiya metodunu) seçmək üçün** istifadə olunan parametrdir. Bu, modelin **maksimum ehtimallıq funksiyasını optimallaşdırmaq üçün** hansı üsulu tətbiq edəcəyini müəyyən edir.  \n",
    "\n",
    "### **Scikit-learn-də mövcud olan solver-lər və onların istifadəsi**  \n",
    "\n",
    "| **Solver**     | **Xüsusiyyətləri** | **Gradient Descent istifadə edir?** |  \n",
    "|--------------|--------------------|--------------------------|  \n",
    "| `liblinear`  | Kiçik datasetlər üçün idealdır, L1 və L2 regularization dəstəkləyir. | ❌ (Coordinate Descent istifadə edir) |  \n",
    "| `lbfgs`      | Newton əsaslı metod, çoxdərəcəli problemlərdə yaxşı işləyir. | ❌ (Quasi-Newton metodudur) |  \n",
    "| `newton-cg`  | Newton’s Conjugate Gradient metodu, L2 dəstəkləyir. | ❌ (İkinci tərtib metodudur) |  \n",
    "| `sag`        | **Stochastic Average Gradient Descent**, böyük datasetlər üçün effektivdir. | ✅ (GD əsaslıdır) |  \n",
    "| `saga`       | **SGD-nin təkmilləşdirilmiş versiyası**, həm L1, həm L2 regularization dəstəkləyir. | ✅ (GD əsaslıdır) |  \n",
    "\n",
    "### **Nəticə:**  \n",
    "Əgər **Gradient Descent əsaslı optimallaşdırma** istəyirsinizsə, `solver='sag'` və ya `solver='saga'` seçməlisiniz.  \n",
    "Digər solver-lər isə ya **Newton əsaslı iterativ metodlar**, ya da **Coordinate Descent** istifadə edirlər.\n",
    "\n",
    "---\n",
    "\n",
    "Bu formatlaşdırma istədiyin kimi oldu mu? Başqa dəyişikliklər etmək istəyirsənsə, mənə bildirə bilərsən!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a117f78f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c1349be",
   "metadata": {},
   "source": [
    "## saga\tSGD-nin təkmilləşdirilmiş versiyası, həm L1, həm L2 regularization dəstəkləyir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de08b8b0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 🔍 **`saga` nədir?**\n",
    "`saga` – **Gradient Descent** metodunun **təkmilləşdirilmiş versiyasıdır**. Əsas məqsədi:  \n",
    "- **Böyük datasetlərdə** daha sürətli və sabit konvergensiya (nəticəyə çatmaq),\n",
    "- **L1 və L2 cərimələndirmə** (regularization) üsullarını dəstəkləmək.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ **SGD nədir?**\n",
    "**SGD (Stochastic Gradient Descent)** – modelin parametrlərini optimallaşdırmaq üçün istifadə edilən metoddur. Klassik GD hər dəfə bütün dataset üzərində işləyir, amma:\n",
    "- **SGD** hər dəfə yalnız **bir nümunəyə** və ya **mini-batch**-ə baxaraq daha sürətli irəliləyir.\n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 `saga` necə təkmilləşib?\n",
    "- `saga`, **SGD-nin zəif cəhətlərini** aradan qaldırmaq üçün yaradılıb.\n",
    "- Daha **sürətli konvergensiya** (yəni nəticəyə daha tez çatır),\n",
    "- **Stabil** və **qərarsızlığa qarşı dözümlüdür**.\n",
    "- **Spars (az sıxlıqlı)** data ilə çox yaxşı işləyir.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **L1 və L2 Regularization nədir?**\n",
    "Regularization – modelin **çox öyrənməsinin (overfitting)** qarşısını almaq üçün tətbiq olunan texnikadır.\n",
    "\n",
    "| Regularization | İzah | Təsiri |\n",
    "|----------------|------|--------|\n",
    "| **L1 (Lasso)** | Parametrlərin cəminə cərimə əlavə edir | Bəzi koeffisiyentləri **0** edir (feature selection) |\n",
    "| **L2 (Ridge)** | Parametrlərin kvadrat cəminə cərimə əlavə edir | Bütün koeffisiyentləri **kiçildər**, amma 0 etməz |\n",
    "\n",
    "🟡 `saga` həm L1, həm də L2 regularization-u **dəstəklədiyi üçün çox çevikdir**. Məsələn:\n",
    "```python\n",
    "LogisticRegression(solver='saga', penalty='l1')  # Lasso\n",
    "LogisticRegression(solver='saga', penalty='l2')  # Ridge\n",
    "LogisticRegression(solver='saga', penalty='elasticnet', l1_ratio=0.5)  # L1 + L2 qarışığı\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Nəticə:\n",
    "`saga` – Gradient Descent-in **ən güclü və çevik versiyalarından biridir**, böyük datasetlər və müxtəlif regularization üsulları ilə rahat işləyir. Ona görə də `scikit-learn`-də **ən professional seçimlərdən biridir.**\n",
    "\n",
    "Əgər istəsən, L1 və L2-nin qrafik təsirini də göstərə bilərəm.\n",
    "\n",
    "---\n",
    "\n",
    "Bu versiya istədiyin şəkildə oldu mu? Başqa dəyişiklik etmək istəyirsənsə, mənə bildirə bilərsən!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f6ce07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
