{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73905d1",
   "metadata": {},
   "source": [
    "## Analitik h…ôll yoxdur n…ô dem…ôkdir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cb3ccd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> **\"Analitik h…ôll yoxdur\" ifad…ôsi o dem…ôkdir ki, m…ôs…ôl…ônin qapalƒ± (closed-form) riyazi h…ôlli yoxdur v…ô ya tapmaq m√ºmk√ºn deyil.**  \n",
    "\n",
    "### Daha d…ôqiq izah:\n",
    "Analitik h…ôll dedikd…ô, funksiyanƒ±n maksimum v…ô ya minimum n√∂qt…ôsini tapan birba≈üa riyazi d√ºstur n…ôz…ôrd…ô tutulur. M…ôs…ôl…ôn, kvadrat t…ônliyin k√∂kl…ôrini tapmaq √º√ß√ºn a≈üaƒüƒ±dakƒ± analitik d√ºsturu istifad…ô edirik:  \n",
    "\n",
    "$$\n",
    "x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n",
    "$$\n",
    "\n",
    "Bu halda biz…ô iterasiya (t…ôkrarlama) lazƒ±m olmur, √ß√ºnki h…ôll birba≈üa d√ºsturla tapƒ±lƒ±r.\n",
    "\n",
    "### N…ô vaxt analitik h…ôll olmur?\n",
    "1. **M√ºr…ôkk…ôb Funksiyalar:** √áox d…ôyi≈ü…ônli v…ô ya qeyri-x…ôtti funksiyalar √º√ß√ºn t√ºrevini sƒ±fƒ±ra b…ôrab…ôr edib …ôl il…ô h…ôll tapmaq √ß…ôtindir.  \n",
    "2. **Qeyri-x…ôtti Funksiyalar:** M…ôs…ôl…ôn, **neyral ≈ü…ôb…ôk…ôl…ôrd…ô** v…ô ya **b√∂y√ºk √∂l√ß√ºl√º optimizasiya probleml…ôrind…ô** diferensial t…ônlikl…ôri …ôl il…ô h…ôll etm…ôk m√ºmk√ºn olmur.  \n",
    "3. **Veril…ônl…ôrin √áoxluƒüu:** Minl…ôrl…ô v…ô ya milyonlarla d…ôyi≈ü…ôn olan real probleml…ôrd…ô analitik h…ôll tapmaq praktiki c…ôh…ôtd…ôn m√ºmk√ºn olmur.\n",
    "\n",
    "### Gradient Descent Niy…ô Lazƒ±mdƒ±r?\n",
    "Analitik h…ôll m√ºmk√ºn olmayanda, gradient descent iterativ bir metod kimi t…ôdric…ôn optimal n√∂qt…ôy…ô yaxƒ±nla≈üƒ±r. Y…ôni, funksiyanƒ±n enm…ô istiqam…ôtini tapƒ±b, h…ôr addƒ±mda azca d…ôyi≈ü…ôr…ôk minimuma (v…ô ya maksimuma) doƒüru h…ôr…ôk…ôt edirik.\n",
    "\n",
    "#### N√ºmun…ô:\n",
    "- **Analitik h…ôlli olan model:** X…ôtti regresiya (normal equation il…ô h…ôll ed…ô bil…ôrik).  \n",
    "- **Analitik h…ôlli olmayan model:** Neyron ≈ü…ôb…ôk…ôl…ôr, qeyri-x…ôtti regresiya modell…ôri.\n",
    "\n",
    "Bu s…ôb…ôbd…ôn **gradient descent** istifad…ô olunur, √ß√ºnki o, iterativ yana≈üma il…ô minimumu t…ôdric…ôn tapƒ±r. üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9017264f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b128dba",
   "metadata": {},
   "source": [
    "## Qapalƒ± (closed-form) riyazi h…ôlli yoxdur\" n…ô dem…ôkdir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6542955c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### üîç \"Qapalƒ± (closed-form) riyazi h…ôlli yoxdur\" n…ô dem…ôkdir?\n",
    "\n",
    "Bu o dem…ôkdir ki, **problemin cavabƒ±nƒ± birba≈üa riyazi d√ºsturla ifad…ô etm…ôk olmur** ‚Äî y…ôni n…ôtic…ôni **c…ôbr (algebra), k√∂k, loqarifm, triqonometriya kimi tanƒ±nmƒ±≈ü riyazi funksiyalarla ifad…ô ed…ô bilm…ôrik**.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå M…ôs…ôl…ôn:\n",
    "\n",
    "#### ‚úÖ Qapalƒ± (closed-form) h…ôlli olan:\n",
    "Kvadrat t…ônlik:\n",
    "$$\n",
    "ax^2 + bx + c = 0\n",
    "$$\n",
    "\n",
    "Onun h…ôlli bel…ôdir:\n",
    "$$\n",
    "x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n",
    "$$\n",
    "\n",
    "Bu **closed-form** h…ôlldir, √ß√ºnki d√ºsturla birba≈üa cavabƒ± tapa bil…ôrik.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ùå Qapalƒ± h…ôlli olmayan:\n",
    "$$\n",
    "x = e^{-x}\n",
    "$$\n",
    "\n",
    "Bu t…ônliyin cavabƒ±nƒ± **…ôld…ôki klassik riyazi simvollarla ifad…ô etm…ôk m√ºmk√ºn deyil**. Y…ôni, **qapalƒ± forma** yoxdur.\n",
    "\n",
    "Bu halda **sayƒ±sal metodlar** (m…ôs…ôl…ôn, Newton-Raphson, Gradient Descent v…ô s.) istifad…ô ed…ôr…ôk t…ôqribi cavab tapƒ±lƒ±r.\n",
    "\n",
    "---\n",
    "\n",
    "### üìò Bir ne√ß…ô …ôsas fikir:\n",
    "- **Qapalƒ± h…ôll**: Birba≈üa d√ºsturla cavab tapƒ±lƒ±r. (tez, sad…ô)\n",
    "- **Qapalƒ± h…ôll yoxdur**: Cavab d√ºsturla tapƒ±la bilmir, **t…ôkrarlanan (iterativ) √ºsullar** il…ô yaxƒ±nla≈üaraq tapƒ±lƒ±r.\n",
    "- Bel…ô hallarda, m…ôs…ôl…ôn **gradient descent**, **numerical optimization**, **approximation methods** istifad…ô olunur.\n",
    "\n",
    "---\n",
    "\n",
    "∆èg…ôr ist…ôs…ôn, konkret misalla da izah ed…ô bil…ôr…ôm. Amma √ºmumi fikir budur ki:  \n",
    "> \"Qapalƒ± h…ôll yoxdur\" dem…ôk, **‚Äúbir d√ºsturla cavabƒ± ifad…ô ed…ô bilmirik‚Äù** dem…ôkdir. Ona g√∂r…ô d…ô, komp√ºterl…ô v…ô ya iterasiya il…ô cavab tapmalƒ±yƒ±q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a879b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69d75811",
   "metadata": {},
   "source": [
    "## Analitik h…ôll n…ô vaxt olur v…ô n…ô vaxt olmur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b96adbc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "B…ôli, Gradient Descent d…ô …ôslind…ô t√∂r…ôm…ôni istifad…ô edir. Amma f…ôrq ondadƒ±r ki, **t√∂r…ôm…ôni sƒ±fƒ±ra b…ôrab…ôr edib analitik h…ôll tapmaq m√ºmk√ºn olmur** v…ô ya **√ßox √ß…ôtindir**. G…ôlin bunu daha detallƒ± anlayaq.  \n",
    "\n",
    "### üåü Analitik h…ôll n…ô vaxt olur?\n",
    "∆èg…ôr bir funksiyanƒ±n t√∂r…ôm…ôsini sƒ±fƒ±ra b…ôrab…ôr edib, **birba≈üa x √º√ß√ºn h…ôll ed…ô biliriks…ô**, dem…ôli, analitik h…ôll var. M…ôs…ôl…ôn, **x…ôtti** funksiyalar √º√ß√ºn bu m√ºmk√ºnd√ºr.  \n",
    "\n",
    "#### X…ôtti regresiya n√ºmun…ôsi:\n",
    "$$\n",
    "J(\\theta) = \\sum (y_i - \\theta_0 - \\theta_1 x_i)^2\n",
    "$$\n",
    "\n",
    "Bu funksiyanƒ± **$\\theta_0$ v…ô $\\theta_1$-…ô g√∂r…ô t√∂r…ôm…ô alƒ±b sƒ±fƒ±ra b…ôrab…ôr ets…ôk**, **Norm Equation** adlƒ± analitik h…ôlli tapa bil…ôrik:\n",
    "\n",
    "$$\n",
    "\\theta = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "Burada **birba≈üa h…ôll var, ona g√∂r…ô Gradient Descent lazƒ±m deyil**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Analitik h…ôll niy…ô h…ôr zaman olmur?\n",
    "1. **Qeyri-x…ôtti funksiyalar** ‚Üí T√∂r…ôm…ôni sƒ±fƒ±ra b…ôrab…ôr edib **…ôl il…ô h…ôll etm…ôk m√ºmk√ºn olmur**.  \n",
    "   M…ôs…ôl…ôn, **logistik regresiya** √º√ß√ºn norm equation yoxdur, √ß√ºnki funksiya qeyri-x…ôttidir.  \n",
    "\n",
    "2. **√áox d…ôyi≈ü…ônli v…ô kompleks modell…ôr** ‚Üí Neyron ≈ü…ôb…ôk…ôl…ôrd…ô v…ô ya d…ôrin √∂yr…ônm…ôd…ô **minl…ôrl…ô parametrl…ôr var** v…ô **bir t…ônlikl…ô h…ôll tapmaq m√ºmk√ºn olmur**.  \n",
    "\n",
    "3. **T…ôrs matris problemi** ‚Üí Norm Equation metodunda $(X^T X)^{-1}$ istifad…ô olunur. **∆èg…ôr matris t…ôrs √ßevrilm…ôzdirs…ô, analitik h…ôll alƒ±nmƒ±r.**  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Gradient Descent n…ô edir?\n",
    "∆èg…ôr **analitik h…ôll yoxdur v…ô ya √ß…ôtindirs…ô**, Gradient Descent **funksiyanƒ±n minimumuna yaxƒ±nla≈ümaq √º√ß√ºn iterativ metod kimi i≈ül…ôyir**. Y…ôni:  \n",
    "- **T√∂r…ôm…ô sƒ±fƒ±ra b…ôrab…ôr edib h…ôll axtarmƒ±rƒ±q**, √ß√ºnki bu √ß…ôtindir.  \n",
    "- ∆èksin…ô, **t√∂r…ôm…ônin i≈üar…ôsini** istifad…ô edirik v…ô **addƒ±m-addƒ±m** enirik:\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\alpha \\cdot \\frac{dJ}{d\\theta}\n",
    "$$\n",
    "\n",
    "Burada $\\alpha$ learning rate-dir.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Son n…ôtic…ô:\n",
    "‚úî **Analitik h…ôll olsa, t√∂r…ôm…ôni sƒ±fƒ±ra b…ôrab…ôr edib tapmaq olar.**  \n",
    "‚úî **Analitik h…ôll olmasa, Gradient Descent iterativ yaxƒ±nla≈üƒ±r.**  \n",
    "‚úî **Gradient Descent d…ô t√∂r…ôm…ô istifad…ô edir, amma onu sƒ±fƒ±ra b…ôrab…ôr edib h…ôll axtarmƒ±r, …ôv…ôzin…ô azaldaraq minimuma yaxƒ±nla≈üƒ±r.**  \n",
    "\n",
    "Bax, f…ôrq budur! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ef19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2c3c2b1",
   "metadata": {},
   "source": [
    "# Analitik h…ôll olub-olmamasƒ±nƒ±n Qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôr il…ô …ôlaq…ôsi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84655251",
   "metadata": {},
   "source": [
    "## Qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôri tapmaq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e656aadd",
   "metadata": {},
   "source": [
    "Qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôri tapmaq √º√ß√ºn …ôvv…ôlc…ô birinci qism…ôn t√∂r…ôm…ôl…ôri, sonra is…ô ikinci d…ôr…ôc…ôli qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôri hesablayƒ±rƒ±q. Verilmi≈ü funksiyanƒ± yenid…ôn qeyd ed…ôk:  \n",
    "\n",
    "$$\n",
    "f(x, y) = x^3 y^2 + 5xy^3\n",
    "$$\n",
    "\n",
    "### 1. **$ x $-…ô g√∂r…ô qism…ôn t√∂r…ôm…ô** (artƒ±q tapƒ±lmƒ±≈üdƒ±):\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = 3x^2 y^2 + 5y^3\n",
    "$$\n",
    "\n",
    "Bunu **$ y $-y…ô g√∂r…ô** t√∂r…ôts…ôk:\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial}{\\partial y} (3x^2 y^2 + 5y^3)\n",
    "$$\n",
    "- $ 3x^2 y^2 $ ifad…ôsinin $ y $-y…ô g√∂r…ô t√∂r…ôm…ôsi: $ 6x^2 y $\n",
    "- $ 5y^3 $ ifad…ôsinin $ y $-y…ô g√∂r…ô t√∂r…ôm…ôsi: $ 15y^2 $\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} = 6x^2 y + 15y^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **$ y $-y…ô g√∂r…ô qism…ôn t√∂r…ôm…ô** (artƒ±q tapƒ±lmƒ±≈üdƒ±):\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial y} = 2x^3 y + 15xy^2\n",
    "$$\n",
    "\n",
    "Bunu **$ x $-…ô g√∂r…ô** t√∂r…ôts…ôk:\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial x} (2x^3 y + 15xy^2)\n",
    "$$\n",
    "- $ 2x^3 y $ ifad…ôsinin $ x $-…ô g√∂r…ô t√∂r…ôm…ôsi: $ 6x^2 y $\n",
    "- $ 15xy^2 $ ifad…ôsinin $ x $-…ô g√∂r…ô t√∂r…ôm…ôsi: $ 15y^2 $\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial x \\partial y} = 6x^2 y + 15y^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **N…ôtic…ô**  \n",
    "Simmetriya qaydasƒ±na g√∂r…ô qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôr eynidir:\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial^2 f}{\\partial x \\partial y} = 6x^2 y + 15y^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565bd745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "529f8e26",
   "metadata": {},
   "source": [
    "##  Qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôr eynidirs…ô"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6e4d3a",
   "metadata": {},
   "source": [
    "Qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôr eynidirs…ô, **analitik h…ôll** var y…ôni  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial^2 f}{\\partial x \\partial y}\n",
    "$$\n",
    "\n",
    "bu o dem…ôkdir ki, funksiyanƒ±n ikinci d…ôr…ôc…ôli qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôri **Clairautun teoremin…ô** g√∂r…ô b…ôrab…ôrdir v…ô $ f(x, y) $ **funksiyasƒ± kifay…ôt q…ôd…ôr hamar (davamlƒ± ikinci d…ôr…ôc…ôli t√∂r…ôm…ôl…ôr…ô malik) bir funksiyadƒ±r**.  \n",
    "\n",
    "Bu is…ô o dem…ôkdir ki, funksiyanƒ± analitik √ºsullarla t…ôdqiq etm…ôk m√ºmk√ºnd√ºr v…ô diferensial t…ônlikl…ôrd…ô t…ôtbiq oluna bil…ôr. Y…ôni, **bu funksiyanƒ± t…ôhlil v…ô inteqrasiya etm…ôkd…ô he√ß bir problem yoxdur**.\n",
    "\n",
    "---\n",
    "\n",
    "**Qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôr** eynidirs…ô **Gradient Descent** lazƒ±m deyil!  \n",
    "\n",
    "√á√ºnki funksiyanƒ±n **t√∂r…ôm…ôl…ôri analitik ≈ü…ôkild…ô tapƒ±lƒ±r v…ô hesablamaq asandƒ±r**. Gradient Descent …ôsas…ôn:  \n",
    "- T√∂r…ôm…ôl…ôri analitik hesablamaq √ß…ôtin v…ô ya m√ºmk√ºn olmadƒ±qda,  \n",
    "- √áox √∂l√ß√ºl√º v…ô m√ºr…ôkk…ôb funksiyalar √º√ß√ºn optimal h…ôll tapmaq lazƒ±m g…ôldikd…ô istifad…ô olunur.  \n",
    "\n",
    "Bu halda, sad…ô **analitik t√∂r…ôm…ô** il…ô lokal v…ô qlobal minimum/maksimumu tapa bil…ôrik. ∆èg…ôr kritik n√∂qt…ôl…ôri tapmaq lazƒ±mdƒ±rsa,  \n",
    "\n",
    "$$\n",
    "\\nabla f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right) = (0,0)\n",
    "$$\n",
    "\n",
    "b…ôrab…ôrliyini h…ôll ed…ôr…ôk stasionar n√∂qt…ôl…ôri tapa bil…ôrik. ∆èg…ôr m…ôqs…ôdiniz budursa, hesablaya bil…ôr…ôm. üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eac303c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b75b0899",
   "metadata": {},
   "source": [
    "## Qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôr b…ôrab…ôrdirs…ô Gradient Descent lazƒ±m deyil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27773210",
   "metadata": {},
   "source": [
    "**Gradient Descent**-…ô ehtiyac olub-olmamasƒ± …ôsas…ôn **funksiyanƒ±n t√∂r…ôm…ôl…ôrinin hesablana bilm…ô v…ôziyy…ôtind…ôn v…ô optimalla≈üdƒ±rma ehtiyacƒ±ndan** asƒ±lƒ±dƒ±r.  \n",
    "\n",
    "### 1Ô∏è‚É£ **Qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôr b…ôrab…ôrdirs…ô Gradient Descent lazƒ±m deyil**  \n",
    "∆èg…ôr qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôr **analitik ≈ü…ôkild…ô** hesablanƒ±bsa v…ô a√ßƒ±q formada ifad…ô edil…ô bilirs…ô, **Gradient Descent lazƒ±m deyil**. √á√ºnki optimal n√∂qt…ôl…ôri **stasionar n√∂qt…ôl…ôri tapƒ±b** analiz ed…ôr…ôk m√º…ôyy…ôn ed…ô bil…ôrik.  \n",
    "\n",
    "**Misal:**  \n",
    "∆èg…ôr $ f(x, y) = x^3 y^2 + 5xy^3 $ √º√ß√ºn qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôr:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial^2 f}{\\partial x \\partial y} = 6x^2 y + 15y^2\n",
    "$$\n",
    "\n",
    "kimi **sad…ô v…ô hesablana bil…ôn** ifad…ôl…ôrdirs…ô, **Gradient Descent lazƒ±m deyil**, √ß√ºnki biz onlarƒ± **t…ônlik kimi h…ôll ed…ô bil…ôrik**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ **Gradient Descent n…ô vaxt lazƒ±mdƒ±r?**  \n",
    "∆èg…ôr qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôr:  \n",
    "‚úÖ **√áox m√ºr…ôkk…ôbdirs…ô** v…ô **…ôl il…ô h…ôlli √ß…ôtindirs…ô**,  \n",
    "‚úÖ **Qeyri-x…ôtti diferensial t…ônlikl…ôr verirs…ô** v…ô **algebraik olaraq h…ôll edil…ô bilmirs…ô**,  \n",
    "‚úÖ **H…ôlli √º√ß√ºn iterativ metodlar t…ôl…ôb olunursa**,  \n",
    "\n",
    "bu zaman **Gradient Descent v…ô ya dig…ôr optimalla≈üdƒ±rma √ºsullarƒ±** lazƒ±m ola bil…ôr.  \n",
    "\n",
    "**Misal:**  \n",
    "∆èg…ôr $ f(x, y) $ el…ô bir funksiya olsaydƒ± ki, onun t√∂r…ôm…ôl…ôri √ßox m√ºr…ôkk…ôb bir sistem …ôm…ôl…ô g…ôtirirdi:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = e^{x^2 + y^2} (2x + 3y^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial y} = \\ln(x^2 + y^2) (x - 4y^3)\n",
    "$$\n",
    "\n",
    "bu halda **…ôl il…ô h…ôll √ßox √ß…ôtin olardƒ±** v…ô **Gradient Descent v…ô ya Newton‚Äôs Method** kimi optimalla≈üdƒ±rma √ºsullarƒ± t…ôtbiq edilm…ôli olardƒ±.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ **Yekun N…ôtic…ô**  \n",
    "üí° **∆èg…ôr qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôr a√ßƒ±q ≈ü…ôkild…ô ifad…ô edilirs…ô v…ô onlarƒ±n sƒ±fƒ±ra b…ôrab…ôr olduƒüu n√∂qt…ôl…ôri tapmaq m√ºmk√ºnd√ºrs…ô, Gradient Descent lazƒ±m deyil.**  \n",
    "\n",
    "üîç **∆èg…ôr qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôr √ßox m√ºr…ôkk…ôb v…ô ya qeyri-x…ôtti bir sistem …ôm…ôl…ô g…ôtirirs…ô, Gradient Descent kimi iterativ metodlar istifad…ô edil…ô bil…ôr.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a781d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21a001d1",
   "metadata": {},
   "source": [
    "## Qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôrin sƒ±fƒ±r olmasƒ±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a40ec5c",
   "metadata": {},
   "source": [
    "**Qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôrin sƒ±fƒ±r olmasƒ± analitik h…ôllin olmamasƒ± dem…ôk deyil.**  \n",
    "\n",
    "∆èslind…ô, qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôrin sƒ±fƒ±r olmasƒ± funksiyanƒ±n m√º…ôyy…ôn istiqam…ôtl…ôrd…ô **d…ôyi≈üm…ôm…ôsi** v…ô ya **d√ºz x…ôtt boyunca sabit qalmasƒ±** kimi hallarƒ± ifad…ô ed…ô bil…ôr. Bu, h…ômi≈ü…ô analitik h…ôllin olmamasƒ± anlamƒ±na g…ôlmir. Daha d…ôqiq izah ed…ôk:  \n",
    "\n",
    "---  \n",
    "\n",
    "## **1Ô∏è‚É£ Qarƒ±≈üƒ±q T√∂r…ôm…ô v…ô Onun Sƒ±fƒ±r Olmasƒ±**  \n",
    "Qarƒ±≈üƒ±q t√∂r…ôm…ô dedikd…ô, bir funksiyanƒ±n iki d…ôyi≈ü…ôn…ô g√∂r…ô ikinci d…ôr…ôc…ôli t√∂r…ôm…ôsi n…ôz…ôrd…ô tutulur:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial x \\partial y}\n",
    "$$  \n",
    "\n",
    "∆èg…ôr bu d…ôy…ôr **b√ºt√ºn n√∂qt…ôl…ôrd…ô sƒ±fƒ±rdƒ±rsa**, bu o dem…ôkdir ki, $ f(x, y) $-nin d…ôyi≈üm…ô s√ºr…ôti **x v…ô y d…ôyi≈ü…ônl…ôri √ºzr…ô qar≈üƒ±lƒ±qlƒ± t…ôsir g√∂st…ôrmir**.  \n",
    "\n",
    "Bu, iki m√ºmk√ºn v…ôziyy…ôti g√∂st…ôrir:  \n",
    "- Funksiya **ayrƒ±lƒ±qda iki d…ôyi≈ü…ôn…ô g√∂r…ô x…ôtti ola bil…ôr**, m…ôs…ôl…ôn:    \n",
    "  $$\n",
    "  f(x, y) = ax + by + c\n",
    "  $$  \n",
    "  Bu halda funksiya √ßox sad…ôdir v…ô **analitik h…ôll tam m√ºmk√ºnd√ºr**.  \n",
    "- ∆èg…ôr funksiya daha m√ºr…ôkk…ôb bir qurulu≈üa malikdirs…ô v…ô **stasionar n√∂qt…ôl…ôr…ô malikdirs…ô**, iterativ metodlar (m…ôs…ôl…ôn, **Gradient Descent**) t…ôl…ôb oluna bil…ôr.  \n",
    "\n",
    "---  \n",
    "\n",
    "## **2Ô∏è‚É£ Qarƒ±≈üƒ±q T√∂r…ôm…ôl…ôrin Sƒ±fƒ±r Olmasƒ± Analitik H…ôllin Olmamasƒ± Dem…ôkdirmi?**  \n",
    "- **Xeyr, h…ôr zaman yox!**    \n",
    "  Qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôrin sƒ±fƒ±r olmasƒ± funksiyanƒ±n **ayrƒ±lƒ±qda $ x $ v…ô $ y $-y…ô g√∂r…ô asanlƒ±qla h…ôll edil…ô bil…ôc…ôyini** g√∂st…ôr…ô bil…ôr. Bel…ô hallarda analitik h…ôll m√∂vcuddur v…ô iterativ metodlara ehtiyac olmur.  \n",
    "\n",
    "- **B…ôz…ôn iterativ metodlar t…ôl…ôb oluna bil…ôr.**    \n",
    "  ∆èg…ôr funksiya **t√∂r…ôm…ôl…ôri sƒ±fƒ±r ed…ôn bir n√∂qt…ôy…ô malikdirs…ô v…ô h…ômin n√∂qt…ôd…ô ikinci d…ôr…ôc…ôli t√∂r…ôm…ôl…ôr funksiyanƒ±n tipliliyini (minimum, maksimum, saddle point) tam m√º…ôyy…ôn ed…ô bilmirs…ô**, Gradient Descent kimi **iterativ metodlar** istifad…ô edil…ô bil…ôr.  \n",
    "\n",
    "---  \n",
    "\n",
    "## **3Ô∏è‚É£ N√ºmun…ôl…ôr il…ô ƒ∞zah**  \n",
    "### **A. Analitik h…ôllin m√ºmk√ºn olduƒüu hallar**  \n",
    "∆èg…ôr $ f(x, y) $ a≈üaƒüƒ±dakƒ± kimi sad…ôdirs…ô:  \n",
    "\n",
    "$$\n",
    "f(x, y) = 3x + 5y\n",
    "$$  \n",
    "\n",
    "Bu funksiyanƒ±n t√∂r…ôm…ôl…ôri bel…ô olacaq:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = 3, \\quad \\frac{\\partial f}{\\partial y} = 5, \\quad \\frac{\\partial^2 f}{\\partial x \\partial y} = 0\n",
    "$$  \n",
    "\n",
    "Burada qarƒ±≈üƒ±q t√∂r…ôm…ô sƒ±fƒ±rdƒ±r, amma funksiya **sad…ô x…ôttidir v…ô analitik h…ôll a√ßƒ±q ≈ü…ôkild…ô m√ºmk√ºnd√ºr.** Burada he√ß bir optimalla≈üdƒ±rma v…ô Gradient Descent t…ôl…ôb olunmur.  \n",
    "\n",
    "---  \n",
    "\n",
    "### **B. Gradient Descent T…ôl…ôb Edil…ôn Hal**  \n",
    "∆èg…ôr funksiyanƒ±z bel…ôdirs…ô:  \n",
    "\n",
    "$$\n",
    "f(x, y) = x^2 y^2 + 5xy\n",
    "$$  \n",
    "\n",
    "Bunun qarƒ±≈üƒ±q t√∂r…ôm…ôsini tapaq:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y} (2xy^2 + 5y) = 4xy + 5\n",
    "$$  \n",
    "\n",
    "Bu, m√º…ôyy…ôn n√∂qt…ôl…ôrd…ô sƒ±fƒ±r ola bil…ôr:  \n",
    "\n",
    "$$\n",
    "4xy + 5 = 0 \\quad \\Rightarrow \\quad xy = -\\frac{5}{4}\n",
    "$$  \n",
    "\n",
    "Bu halda, **stasionar n√∂qt…ôl…ôri tapmaq √º√ß√ºn Gradient Descent kimi metodlara ehtiyac ola bil…ôr**, √ß√ºnki funksiyanƒ±n **qeyri-x…ôtti olmasƒ± v…ô √ßoxlu lokal ekstremumlara malik olmasƒ± m√ºmk√ºnd√ºr.**  \n",
    "\n",
    "---  \n",
    "\n",
    "## **4Ô∏è‚É£ Son N…ôtic…ô**  \n",
    "- **Qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôrin sƒ±fƒ±r olmasƒ± h…ômi≈ü…ô analitik h…ôllin olmamasƒ± dem…ôk deyil.**    \n",
    "- **∆èg…ôr funksiya sad…ôdirs…ô, analitik h…ôll m√ºmk√ºnd√ºr.**    \n",
    "- **∆èg…ôr funksiya m√ºr…ôkk…ôb v…ô qeyri-x…ôttidirs…ô, stasionar n√∂qt…ôl…ôrin tapƒ±lmasƒ± √º√ß√ºn Gradient Descent kimi optimalla≈üdƒ±rma metodlarƒ± lazƒ±m ola bil…ôr.**    \n",
    "\n",
    "∆èg…ôr konkret bir funksiya √ºz…ôrind…ô ara≈üdƒ±rma aparmaq ist…ôyirsinizs…ô, n√ºmun…ô verin, birlikd…ô baxaq! üòä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be825ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "077eb842",
   "metadata": {},
   "source": [
    "## Qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôr sƒ±fƒ±rdƒ±rsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54370863",
   "metadata": {},
   "source": [
    " ∆èg…ôr **qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôr sƒ±fƒ±rdƒ±rsa** (y…ôni funksiyanƒ±n gradienti sƒ±fƒ±ra b…ôrab…ôrdirs…ô), o zaman **Gradient Descent** istifad…ô etm…ôyin …ôsas m…ôqs…ôdi **optimal h…ôll…ô √ßatmaqdƒ±r**.  \n",
    "\n",
    "### 1Ô∏è‚É£ **Qarƒ±≈üƒ±q T√∂r…ôm…ôl…ôr Sƒ±fƒ±ra B…ôrab…ôrdirs…ô:**  \n",
    "\n",
    "∆èg…ôr $ \\nabla f(x, y) = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right) = (0, 0) $, y…ôni funksiyanƒ±n gradienti sƒ±fƒ±rdƒ±rsa, bu, **stasionar n√∂qt…ô** olduƒüunu bildirir. Ancaq bu n√∂qt…ô **minimum**, **maksimum** v…ô ya **saddl…ô≈ümi≈ü n√∂qt…ô** (saddle point) ola bil…ôr.  \n",
    "\n",
    "Bu halda, **Gradient Descent** istifad…ô etm…ôk vacibdir, √ß√ºnki bu metod **stasionar n√∂qt…ôl…ôri tapmaq √º√ß√ºn iterativ olaraq minimuma yaxƒ±nla≈ümaq** √º√ß√ºn istifad…ô olunur. Gradient Descent-in …ôsas m…ôqs…ôdi **minimum n√∂qt…ôsin…ô doƒüru h…ôr…ôk…ôt etm…ôk**dir, x√ºsus…ôn d…ô analitik h…ôll tapmaq √ß…ôtin v…ô ya m√ºmk√ºn olmadƒ±qda.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ **Gradient Descent nec…ô i≈ül…ôyir?**  \n",
    "Gradient Descent metodunun …ôsas ideyasƒ± funksiyanƒ±n **gradientin…ô …ôsaslanaraq** (y…ôni t√∂r…ôm…ôl…ôrin…ô …ôsaslanaraq) funksiyanƒ± minimuma doƒüru optimalla≈üdƒ±rmaqdƒ±r. H…ôr iterasiyada, cari n√∂qt…ôd…ôn funksiyanƒ±n gradientin…ô …ôsaslanaraq n√∂vb…ôti n√∂qt…ô hesablanƒ±r.  \n",
    "\n",
    "**Gradient Descent formulasƒ±:**  \n",
    "\n",
    "$$\n",
    "\\theta_{new} = \\theta_{old} - \\alpha \\nabla f(\\theta)\n",
    "$$  \n",
    "\n",
    "- $ \\theta $ - parametrl…ôr (burada $ x $ v…ô $ y $ ola bil…ôr)  \n",
    "- $ \\alpha $ - √∂yr…ônm…ô s√ºr…ôti (learning rate)  \n",
    "- $ \\nabla f(\\theta) $ - funksiyanƒ±n gradienti (t√∂r…ôm…ôl…ôri)  \n",
    "\n",
    "Bu t…ônlikd…ô $ \\alpha $ √ßox b√∂y√ºk v…ô ya √ßox ki√ßik se√ßildikd…ô, optimalla≈üdƒ±rma prosesi ya √ßox s√ºr…ôtli, ya da √ßox yava≈ü ola bil…ôr. ∆èg…ôr $ \\alpha $ √ßox b√∂y√ºkd√ºrs…ô, ad…ôt…ôn h…ôllin optimalla≈ümasƒ± t…ôkrarlanmaz, √ß√ºnki h…ôr addƒ±mda √ßox b√∂y√ºk d…ôyi≈üiklikl…ôr ba≈ü ver…ôr.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ **Gradient Descent N…ô Vaxt T…ôtbiq Edilir?**  \n",
    "Gradient Descent a≈üaƒüƒ±dakƒ± hallarda istifad…ô olunur:  \n",
    "\n",
    "- **Funksiyanƒ±n analitik olaraq h…ôlli m√ºmk√ºn deyil** v…ô ya √ßox m√ºr…ôkk…ôbdir (funksiyanƒ±n t√∂r…ôm…ôsi v…ô ya sƒ±fƒ±ra b…ôrab…ôr olduƒüu n√∂qt…ôl…ôr …ôl il…ô tapƒ±la bilm…ôz).  \n",
    "- **Funksiya √ßox b√∂y√ºk √∂l√ß√ºl√º v…ô ya qeyri-x…ôtti** olur (m…ôs…ôl…ôn, √ßox sayda d…ôyi≈ü…ôni olan v…ô ya √ßox m√ºr…ôkk…ôb formullarƒ± olan).  \n",
    "- **√áoxsaylƒ± stasionar n√∂qt…ôl…ôr m√∂vcuddur** v…ô optimal h…ôllin tapƒ±lmasƒ± √ß…ôtindir (minimalla≈üdƒ±rma v…ô ya optimalla≈üdƒ±rma tap≈üƒ±rƒ±qlarƒ±).  \n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ **N…ôtic…ô**  \n",
    "∆èg…ôr qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôr sƒ±fƒ±rdƒ±rsa v…ô funksiyanƒ±n analitik h…ôlli √ß…ôtindirs…ô, **Gradient Descent** istifad…ô etm…ôk √ßox faydalƒ±dƒ±r. Bu metod funksiyanƒ± **iterativ ≈ü…ôkild…ô optimalla≈üdƒ±rmaƒüa** k√∂m…ôk edir v…ô yerli minimumu tapmaq √º√ß√ºn istifad…ô edilir.  \n",
    "\n",
    "Ehtiyacƒ±nƒ±z varsa, **Gradient Descent**-in Python-da t…ôtbiqi √º√ß√ºn n√ºmun…ô ver…ô bil…ôr…ôm! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee768c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6c93ac3",
   "metadata": {},
   "source": [
    "## √úmumil…ô≈üdrim…ô"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d484e",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ **Qarƒ±≈üƒ±q T√∂r…ôm…ôl…ôrin B…ôrab…ôrliyi:**  \n",
    "∆èg…ôr **qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôr b…ôrab…ôrdirs…ô**, y…ôni  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial^2 f}{\\partial x \\partial y},\n",
    "$$  \n",
    "\n",
    "bu, **Clairaut-un teoremi**n…ô g√∂r…ô **funksiyanƒ±n h…ôm $ x $-…ô, h…ôm d…ô $ y $-y…ô g√∂r…ô t√ºrevlerinin m√ºbadil…ôsi m√ºmk√ºn olduƒüunu** bildirir. Bu halda **funksiya hamar (differensiyalanabil…ôn)** sayƒ±lƒ±r v…ô **analitik h…ôll** m√ºmk√ºnd√ºr. Y…ôni, bu halda funksiyanƒ±n **t√∂r…ôm…ôl…ôri m√∂vcuddur v…ô d√ºzg√ºn bir ≈ü…ôkild…ô hesablanƒ±r**.  \n",
    "\n",
    "**Qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôrin b…ôrab…ôr olmasƒ±** funksiyanƒ±n analitik h…ôlli olduƒüu anlamƒ±na g…ôlir, √ß√ºnki bu, funksiyanƒ±n **davamlƒ± v…ô diferensiyalanabil…ôn olduƒüunu** g√∂st…ôrir.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ **Qarƒ±≈üƒ±q T√∂r…ôm…ôl…ôrin Sƒ±fƒ±r Olmasƒ±:**  \n",
    "∆èg…ôr **qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôr sƒ±fƒ±rdƒ±rsa** (y…ôni, bir v…ô ya daha √ßox t√∂r…ôm…ô sƒ±fƒ±ra b…ôrab…ôrdirs…ô), bu, funksiyanƒ±n **stasionar n√∂qt…ôl…ôr…ô sahib olduƒüunu** bildir…ô bil…ôr. Lakin **qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôrin sƒ±fƒ±r olmasƒ±** funksiyanƒ±n **hamar olmamasƒ±** dem…ôk deyil. Funksiya **davamlƒ±** v…ô **diferensiyalanabil…ôn** ola bil…ôr, amma bu, sad…ôc…ô onun **minimum v…ô ya maksimum n√∂qt…ô olduƒüunu** g√∂st…ôrir.  \n",
    "\n",
    "**Qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôrin sƒ±fƒ±r olmasƒ±**, optimalla≈üdƒ±rma prosesind…ô **Gradient Descent** kimi metodlara ehtiyac olduƒüunu g√∂st…ôrir. Bu, funksiyanƒ±n **minimum, maksimum v…ô ya saddle point** (saddl…ô≈ümi≈ü n√∂qt…ô) tapma prosesi il…ô baƒülƒ± ola bil…ôr.  \n",
    "\n",
    "**Qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôrin sƒ±fƒ±r olmasƒ±** funksiyanƒ±n **differensiyalanabil…ôn olduƒüunu**, amma **qeyri-x…ôtti v…ô ya qeyri-analitik olduƒüunu** g√∂st…ôrm…ôz. Bu v…ôziyy…ôtd…ô d…ô analitik h…ôll m√ºmk√ºnd√ºr, amma o h…ôll **iterativ optimalla≈üdƒ±rma √ºsullarƒ± il…ô** tapƒ±la bil…ôr.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ **N…ôtic…ô:**  \n",
    "- **Qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôrin b…ôrab…ôr olmasƒ± (Clairaut-un teoremi)**: Funksiya **hamardƒ±r** v…ô analitik h…ôll m√∂vcuddur.  \n",
    "- **Qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôrin sƒ±fƒ±r olmasƒ±**: Funksiya **differensiyalanabil…ôndir**, lakin bununla yana≈üƒ±, funksiyanƒ±n optimalla≈üdƒ±rƒ±lmasƒ± √º√ß√ºn **iterativ metodlar** (m…ôs…ôl…ôn, Gradient Descent) lazƒ±m ola bil…ôr. Bu, funksiyanƒ±n minimum, maksimum v…ô ya saddle point tapmaq m…ôqs…ôdil…ô edil…ô bil…ôr.  \n",
    "\n",
    "Bel…ôlikl…ô, qarƒ±≈üƒ±q t√∂r…ôm…ôl…ôrin sƒ±fƒ±r olmasƒ± funksiyanƒ±n **hamar olmamasƒ±** deyil, sad…ôc…ô **stasionar n√∂qt…ôl…ôrin** olduƒüunu v…ô optimalla≈üdƒ±rma √ºsullarƒ±nƒ±n lazƒ±m ola bil…ôc…ôyini g√∂st…ôrir.  \n",
    "\n",
    "∆èg…ôr ba≈üqa suallarƒ±nƒ±z varsa, m…ôn…ô bildirin! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b3f49e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a766298",
   "metadata": {},
   "source": [
    "# Analitik h…ôll olub-olmamasƒ±nƒ±n Funksiyanƒ±n t√∂r…ôm…ôsinin k…ôsil…ôn olmasƒ± il…ô …ôlaq…ôsi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585d821",
   "metadata": {},
   "source": [
    "## Funksiyanƒ±n t√∂r…ôm…ôsi k…ôsil…ôndirs…ô, analitik h…ôll olmaya bil…ôr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8029ef1",
   "metadata": {},
   "source": [
    "### M…ôs…ôl…ô n…ôdir?\n",
    "Biz √ßox vaxt bel…ô suallar il…ô qar≈üƒ±la≈üƒ±rƒ±q:\n",
    "\n",
    "> Verilmi≈ü bir funksiya √º√ß√ºn ekstremum n√∂qt…ôl…ôrini tapmaq ist…ôyirik (m…ôs…ôl…ôn, minimum v…ô ya maksimum). Bunun √º√ß√ºn:\n",
    "> 1. ∆èvv…ôlc…ô funksiyanƒ±n t√∂r…ôm…ôsini tapƒ±rƒ±q.\n",
    "> 2. Sonra bu t√∂r…ôm…ôni **sƒ±fƒ±ra b…ôrab…ôr edirik**: $f'(x) = 0$\n",
    "> 3. Bu t…ônliyi h…ôll ed…ôr…ôk kritik n√∂qt…ôl…ôri tapƒ±rƒ±q.\n",
    "\n",
    "---\n",
    "\n",
    "### Problem harada yaranƒ±r?\n",
    "∆èg…ôr funksiyanƒ±n **t√∂r…ôm…ôsi k…ôsil…ôndirs…ô**, y…ôni **h…ôr n√∂qt…ôd…ô diferensial deyils…ô** (t√∂r…ôm…ôsi yoxdur), onda:\n",
    "- Bu metod (t√∂r…ôm…ô = 0 metodu) **b√ºt√ºn h…ôll…ôri verm…ôy…ô bil…ôr**.\n",
    "- H…ôtta **analitik t…ônlik qurmaq da m√ºmk√ºn olmaya bil…ôr**.\n",
    "\n",
    "---\n",
    "\n",
    "### N√ºmun…ô: $f(x) = |x|$\n",
    "Bu √ßox klassik n√ºmun…ôdir. G…ôlin baxaq:\n",
    "\n",
    "- $f(x) = |x|$ funksiyasƒ±nƒ± t√∂r…ôm…ô il…ô ara≈üdƒ±raq:\n",
    "  - $f'(x) = 1$ …ôg…ôr $x > 0$\n",
    "  - $f'(x) = -1$ …ôg…ôr $x < 0$\n",
    "  - Amma **$x = 0$** n√∂qt…ôsind…ô **t√∂r…ôm…ô yoxdur**, √ß√ºnki **k…ôsilir** (sol v…ô saƒü limit f…ôrqlidir).\n",
    "  \n",
    "ƒ∞ndi biz maksimum v…ô ya minimum tapmaq √º√ß√ºn deyirik: $f'(x) = 0$\n",
    "\n",
    "Amma bu t…ônliyin **h…ôlli yoxdur**, √ß√ºnki he√ß bir $x$ n√∂qt…ôsind…ô $f'(x) = 0$ deyil. Halbuki:\n",
    "\n",
    "> Biz √ßox g√∂z…ôl bilirik ki, $f(x) = |x|$ funksiyasƒ±nƒ±n **minimum n√∂qt…ôsi** $x = 0$-dadƒ±r!\n",
    "\n",
    "---\n",
    "\n",
    "### N…ôtic…ô:\n",
    "Bel…ô funksiyalar √º√ß√ºn **analitik √ºsul** ‚Äì y…ôni \"t√∂r…ôm…ô tap, sƒ±fƒ±ra b…ôrab…ôr et v…ô h…ôll et\" yana≈ümasƒ± **i≈ül…ômir**.\n",
    "\n",
    "Buna g√∂r…ô **dey…ô bil…ôrik ki**:\n",
    "> **\"Funksiyanƒ±n t√∂r…ôm…ôsi k…ôsil…ôndirs…ô, analitik h…ôll olmaya bil…ôr.\"**\n",
    "\n",
    "---\n",
    "\n",
    "### ∆èlav…ô qeydl…ôr:\n",
    "- Bel…ô hallarda ya **qrafik…ô baxmaq**, ya **…ôlav…ô analiz √ºsullarƒ±** (subderivative, interval analizi v…ô s.) istifad…ô olunur.\n",
    "- B…ôzi hallarda is…ô **numerik √ºsullarla** yaxƒ±n h…ôll tapmaq lazƒ±m g…ôlir.\n",
    "\n",
    "---\n",
    "\n",
    "ƒ∞st…ôs…ôn bu c√ºr funksiyalarla baƒülƒ± konkret misallar √ºz…ôrind…ô d…ô i≈ül…ôy…ô bil…ôrik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406723ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
