{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73905d1",
   "metadata": {},
   "source": [
    "## Analitik hÉ™ll yoxdur nÉ™ demÉ™kdir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cb3ccd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> **\"Analitik hÉ™ll yoxdur\" ifadÉ™si o demÉ™kdir ki, mÉ™sÉ™lÉ™nin qapalÄ± (closed-form) riyazi hÉ™lli yoxdur vÉ™ ya tapmaq mÃ¼mkÃ¼n deyil.**  \n",
    "\n",
    "### Daha dÉ™qiq izah:\n",
    "Analitik hÉ™ll dedikdÉ™, funksiyanÄ±n maksimum vÉ™ ya minimum nÃ¶qtÉ™sini tapan birbaÅŸa riyazi dÃ¼stur nÉ™zÉ™rdÉ™ tutulur. MÉ™sÉ™lÉ™n, kvadrat tÉ™nliyin kÃ¶klÉ™rini tapmaq Ã¼Ã§Ã¼n aÅŸaÄŸÄ±dakÄ± analitik dÃ¼sturu istifadÉ™ edirik:  \n",
    "\n",
    "$$\n",
    "x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n",
    "$$\n",
    "\n",
    "Bu halda bizÉ™ iterasiya (tÉ™krarlama) lazÄ±m olmur, Ã§Ã¼nki hÉ™ll birbaÅŸa dÃ¼sturla tapÄ±lÄ±r.\n",
    "\n",
    "### NÉ™ vaxt analitik hÉ™ll olmur?\n",
    "1. **MÃ¼rÉ™kkÉ™b Funksiyalar:** Ã‡ox dÉ™yiÅŸÉ™nli vÉ™ ya qeyri-xÉ™tti funksiyalar Ã¼Ã§Ã¼n tÃ¼revini sÄ±fÄ±ra bÉ™rabÉ™r edib É™l ilÉ™ hÉ™ll tapmaq Ã§É™tindir.  \n",
    "2. **Qeyri-xÉ™tti Funksiyalar:** MÉ™sÉ™lÉ™n, **neyral ÅŸÉ™bÉ™kÉ™lÉ™rdÉ™** vÉ™ ya **bÃ¶yÃ¼k Ã¶lÃ§Ã¼lÃ¼ optimizasiya problemlÉ™rindÉ™** diferensial tÉ™nliklÉ™ri É™l ilÉ™ hÉ™ll etmÉ™k mÃ¼mkÃ¼n olmur.  \n",
    "3. **VerilÉ™nlÉ™rin Ã‡oxluÄŸu:** MinlÉ™rlÉ™ vÉ™ ya milyonlarla dÉ™yiÅŸÉ™n olan real problemlÉ™rdÉ™ analitik hÉ™ll tapmaq praktiki cÉ™hÉ™tdÉ™n mÃ¼mkÃ¼n olmur.\n",
    "\n",
    "### Gradient Descent NiyÉ™ LazÄ±mdÄ±r?\n",
    "Analitik hÉ™ll mÃ¼mkÃ¼n olmayanda, gradient descent iterativ bir metod kimi tÉ™dricÉ™n optimal nÃ¶qtÉ™yÉ™ yaxÄ±nlaÅŸÄ±r. YÉ™ni, funksiyanÄ±n enmÉ™ istiqamÉ™tini tapÄ±b, hÉ™r addÄ±mda azca dÉ™yiÅŸÉ™rÉ™k minimuma (vÉ™ ya maksimuma) doÄŸru hÉ™rÉ™kÉ™t edirik.\n",
    "\n",
    "#### NÃ¼munÉ™:\n",
    "- **Analitik hÉ™lli olan model:** XÉ™tti regresiya (normal equation ilÉ™ hÉ™ll edÉ™ bilÉ™rik).  \n",
    "- **Analitik hÉ™lli olmayan model:** Neyron ÅŸÉ™bÉ™kÉ™lÉ™r, qeyri-xÉ™tti regresiya modellÉ™ri.\n",
    "\n",
    "Bu sÉ™bÉ™bdÉ™n **gradient descent** istifadÉ™ olunur, Ã§Ã¼nki o, iterativ yanaÅŸma ilÉ™ minimumu tÉ™dricÉ™n tapÄ±r. ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9017264f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b128dba",
   "metadata": {},
   "source": [
    "## QapalÄ± (closed-form) riyazi hÉ™lli yoxdur\" nÉ™ demÉ™kdir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6542955c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ğŸ” \"QapalÄ± (closed-form) riyazi hÉ™lli yoxdur\" nÉ™ demÉ™kdir?\n",
    "\n",
    "Bu o demÉ™kdir ki, **problemin cavabÄ±nÄ± birbaÅŸa riyazi dÃ¼sturla ifadÉ™ etmÉ™k olmur** â€” yÉ™ni nÉ™ticÉ™ni **cÉ™br (algebra), kÃ¶k, loqarifm, triqonometriya kimi tanÄ±nmÄ±ÅŸ riyazi funksiyalarla ifadÉ™ edÉ™ bilmÉ™rik**.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ MÉ™sÉ™lÉ™n:\n",
    "\n",
    "#### âœ… QapalÄ± (closed-form) hÉ™lli olan:\n",
    "Kvadrat tÉ™nlik:\n",
    "$$\n",
    "ax^2 + bx + c = 0\n",
    "$$\n",
    "\n",
    "Onun hÉ™lli belÉ™dir:\n",
    "$$\n",
    "x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n",
    "$$\n",
    "\n",
    "Bu **closed-form** hÉ™lldir, Ã§Ã¼nki dÃ¼sturla birbaÅŸa cavabÄ± tapa bilÉ™rik.\n",
    "\n",
    "---\n",
    "\n",
    "#### âŒ QapalÄ± hÉ™lli olmayan:\n",
    "$$\n",
    "x = e^{-x}\n",
    "$$\n",
    "\n",
    "Bu tÉ™nliyin cavabÄ±nÄ± **É™ldÉ™ki klassik riyazi simvollarla ifadÉ™ etmÉ™k mÃ¼mkÃ¼n deyil**. YÉ™ni, **qapalÄ± forma** yoxdur.\n",
    "\n",
    "Bu halda **sayÄ±sal metodlar** (mÉ™sÉ™lÉ™n, Newton-Raphson, Gradient Descent vÉ™ s.) istifadÉ™ edÉ™rÉ™k tÉ™qribi cavab tapÄ±lÄ±r.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“˜ Bir neÃ§É™ É™sas fikir:\n",
    "- **QapalÄ± hÉ™ll**: BirbaÅŸa dÃ¼sturla cavab tapÄ±lÄ±r. (tez, sadÉ™)\n",
    "- **QapalÄ± hÉ™ll yoxdur**: Cavab dÃ¼sturla tapÄ±la bilmir, **tÉ™krarlanan (iterativ) Ã¼sullar** ilÉ™ yaxÄ±nlaÅŸaraq tapÄ±lÄ±r.\n",
    "- BelÉ™ hallarda, mÉ™sÉ™lÉ™n **gradient descent**, **numerical optimization**, **approximation methods** istifadÉ™ olunur.\n",
    "\n",
    "---\n",
    "\n",
    "ÆgÉ™r istÉ™sÉ™n, konkret misalla da izah edÉ™ bilÉ™rÉ™m. Amma Ã¼mumi fikir budur ki:  \n",
    "> \"QapalÄ± hÉ™ll yoxdur\" demÉ™k, **â€œbir dÃ¼sturla cavabÄ± ifadÉ™ edÉ™ bilmirikâ€** demÉ™kdir. Ona gÃ¶rÉ™ dÉ™, kompÃ¼terlÉ™ vÉ™ ya iterasiya ilÉ™ cavab tapmalÄ±yÄ±q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a879b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69d75811",
   "metadata": {},
   "source": [
    "## Analitik hÉ™ll nÉ™ vaxt olur vÉ™ nÉ™ vaxt olmur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b96adbc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "BÉ™li, Gradient Descent dÉ™ É™slindÉ™ tÃ¶rÉ™mÉ™ni istifadÉ™ edir. Amma fÉ™rq ondadÄ±r ki, **tÃ¶rÉ™mÉ™ni sÄ±fÄ±ra bÉ™rabÉ™r edib analitik hÉ™ll tapmaq mÃ¼mkÃ¼n olmur** vÉ™ ya **Ã§ox Ã§É™tindir**. GÉ™lin bunu daha detallÄ± anlayaq.  \n",
    "\n",
    "### ğŸŒŸ Analitik hÉ™ll nÉ™ vaxt olur?\n",
    "ÆgÉ™r bir funksiyanÄ±n tÃ¶rÉ™mÉ™sini sÄ±fÄ±ra bÉ™rabÉ™r edib, **birbaÅŸa x Ã¼Ã§Ã¼n hÉ™ll edÉ™ biliriksÉ™**, demÉ™li, analitik hÉ™ll var. MÉ™sÉ™lÉ™n, **xÉ™tti** funksiyalar Ã¼Ã§Ã¼n bu mÃ¼mkÃ¼ndÃ¼r.  \n",
    "\n",
    "#### XÉ™tti regresiya nÃ¼munÉ™si:\n",
    "$$\n",
    "J(\\theta) = \\sum (y_i - \\theta_0 - \\theta_1 x_i)^2\n",
    "$$\n",
    "\n",
    "Bu funksiyanÄ± **$\\theta_0$ vÉ™ $\\theta_1$-É™ gÃ¶rÉ™ tÃ¶rÉ™mÉ™ alÄ±b sÄ±fÄ±ra bÉ™rabÉ™r etsÉ™k**, **Norm Equation** adlÄ± analitik hÉ™lli tapa bilÉ™rik:\n",
    "\n",
    "$$\n",
    "\\theta = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "Burada **birbaÅŸa hÉ™ll var, ona gÃ¶rÉ™ Gradient Descent lazÄ±m deyil**.\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ Analitik hÉ™ll niyÉ™ hÉ™r zaman olmur?\n",
    "1. **Qeyri-xÉ™tti funksiyalar** â†’ TÃ¶rÉ™mÉ™ni sÄ±fÄ±ra bÉ™rabÉ™r edib **É™l ilÉ™ hÉ™ll etmÉ™k mÃ¼mkÃ¼n olmur**.  \n",
    "   MÉ™sÉ™lÉ™n, **logistik regresiya** Ã¼Ã§Ã¼n norm equation yoxdur, Ã§Ã¼nki funksiya qeyri-xÉ™ttidir.  \n",
    "\n",
    "2. **Ã‡ox dÉ™yiÅŸÉ™nli vÉ™ kompleks modellÉ™r** â†’ Neyron ÅŸÉ™bÉ™kÉ™lÉ™rdÉ™ vÉ™ ya dÉ™rin Ã¶yrÉ™nmÉ™dÉ™ **minlÉ™rlÉ™ parametrlÉ™r var** vÉ™ **bir tÉ™nliklÉ™ hÉ™ll tapmaq mÃ¼mkÃ¼n olmur**.  \n",
    "\n",
    "3. **TÉ™rs matris problemi** â†’ Norm Equation metodunda $(X^T X)^{-1}$ istifadÉ™ olunur. **ÆgÉ™r matris tÉ™rs Ã§evrilmÉ™zdirsÉ™, analitik hÉ™ll alÄ±nmÄ±r.**  \n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Gradient Descent nÉ™ edir?\n",
    "ÆgÉ™r **analitik hÉ™ll yoxdur vÉ™ ya Ã§É™tindirsÉ™**, Gradient Descent **funksiyanÄ±n minimumuna yaxÄ±nlaÅŸmaq Ã¼Ã§Ã¼n iterativ metod kimi iÅŸlÉ™yir**. YÉ™ni:  \n",
    "- **TÃ¶rÉ™mÉ™ sÄ±fÄ±ra bÉ™rabÉ™r edib hÉ™ll axtarmÄ±rÄ±q**, Ã§Ã¼nki bu Ã§É™tindir.  \n",
    "- ÆksinÉ™, **tÃ¶rÉ™mÉ™nin iÅŸarÉ™sini** istifadÉ™ edirik vÉ™ **addÄ±m-addÄ±m** enirik:\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\alpha \\cdot \\frac{dJ}{d\\theta}\n",
    "$$\n",
    "\n",
    "Burada $\\alpha$ learning rate-dir.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Son nÉ™ticÉ™:\n",
    "âœ” **Analitik hÉ™ll olsa, tÃ¶rÉ™mÉ™ni sÄ±fÄ±ra bÉ™rabÉ™r edib tapmaq olar.**  \n",
    "âœ” **Analitik hÉ™ll olmasa, Gradient Descent iterativ yaxÄ±nlaÅŸÄ±r.**  \n",
    "âœ” **Gradient Descent dÉ™ tÃ¶rÉ™mÉ™ istifadÉ™ edir, amma onu sÄ±fÄ±ra bÉ™rabÉ™r edib hÉ™ll axtarmÄ±r, É™vÉ™zinÉ™ azaldaraq minimuma yaxÄ±nlaÅŸÄ±r.**  \n",
    "\n",
    "Bax, fÉ™rq budur! ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ef19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2c3c2b1",
   "metadata": {},
   "source": [
    "# Analitik hÉ™ll olub-olmamasÄ±nÄ±n QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r ilÉ™ É™laqÉ™si"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84655251",
   "metadata": {},
   "source": [
    "## QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™ri tapmaq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e656aadd",
   "metadata": {},
   "source": [
    "QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™ri tapmaq Ã¼Ã§Ã¼n É™vvÉ™lcÉ™ birinci qismÉ™n tÃ¶rÉ™mÉ™lÉ™ri, sonra isÉ™ ikinci dÉ™rÉ™cÉ™li qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™ri hesablayÄ±rÄ±q. VerilmiÅŸ funksiyanÄ± yenidÉ™n qeyd edÉ™k:  \n",
    "\n",
    "$$\n",
    "f(x, y) = x^3 y^2 + 5xy^3\n",
    "$$\n",
    "\n",
    "### 1. **$ x $-É™ gÃ¶rÉ™ qismÉ™n tÃ¶rÉ™mÉ™** (artÄ±q tapÄ±lmÄ±ÅŸdÄ±):\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = 3x^2 y^2 + 5y^3\n",
    "$$\n",
    "\n",
    "Bunu **$ y $-yÉ™ gÃ¶rÉ™** tÃ¶rÉ™tsÉ™k:\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial}{\\partial y} (3x^2 y^2 + 5y^3)\n",
    "$$\n",
    "- $ 3x^2 y^2 $ ifadÉ™sinin $ y $-yÉ™ gÃ¶rÉ™ tÃ¶rÉ™mÉ™si: $ 6x^2 y $\n",
    "- $ 5y^3 $ ifadÉ™sinin $ y $-yÉ™ gÃ¶rÉ™ tÃ¶rÉ™mÉ™si: $ 15y^2 $\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} = 6x^2 y + 15y^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **$ y $-yÉ™ gÃ¶rÉ™ qismÉ™n tÃ¶rÉ™mÉ™** (artÄ±q tapÄ±lmÄ±ÅŸdÄ±):\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial y} = 2x^3 y + 15xy^2\n",
    "$$\n",
    "\n",
    "Bunu **$ x $-É™ gÃ¶rÉ™** tÃ¶rÉ™tsÉ™k:\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial x} (2x^3 y + 15xy^2)\n",
    "$$\n",
    "- $ 2x^3 y $ ifadÉ™sinin $ x $-É™ gÃ¶rÉ™ tÃ¶rÉ™mÉ™si: $ 6x^2 y $\n",
    "- $ 15xy^2 $ ifadÉ™sinin $ x $-É™ gÃ¶rÉ™ tÃ¶rÉ™mÉ™si: $ 15y^2 $\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial x \\partial y} = 6x^2 y + 15y^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **NÉ™ticÉ™**  \n",
    "Simmetriya qaydasÄ±na gÃ¶rÉ™ qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r eynidir:\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial^2 f}{\\partial x \\partial y} = 6x^2 y + 15y^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565bd745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "529f8e26",
   "metadata": {},
   "source": [
    "##  QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r eynidirsÉ™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6e4d3a",
   "metadata": {},
   "source": [
    "QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r eynidirsÉ™, **analitik hÉ™ll** var yÉ™ni  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial^2 f}{\\partial x \\partial y}\n",
    "$$\n",
    "\n",
    "bu o demÉ™kdir ki, funksiyanÄ±n ikinci dÉ™rÉ™cÉ™li qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™ri **Clairautun teoreminÉ™** gÃ¶rÉ™ bÉ™rabÉ™rdir vÉ™ $ f(x, y) $ **funksiyasÄ± kifayÉ™t qÉ™dÉ™r hamar (davamlÄ± ikinci dÉ™rÉ™cÉ™li tÃ¶rÉ™mÉ™lÉ™rÉ™ malik) bir funksiyadÄ±r**.  \n",
    "\n",
    "Bu isÉ™ o demÉ™kdir ki, funksiyanÄ± analitik Ã¼sullarla tÉ™dqiq etmÉ™k mÃ¼mkÃ¼ndÃ¼r vÉ™ diferensial tÉ™nliklÉ™rdÉ™ tÉ™tbiq oluna bilÉ™r. YÉ™ni, **bu funksiyanÄ± tÉ™hlil vÉ™ inteqrasiya etmÉ™kdÉ™ heÃ§ bir problem yoxdur**.\n",
    "\n",
    "---\n",
    "\n",
    "**QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r** eynidirsÉ™ **Gradient Descent** lazÄ±m deyil!  \n",
    "\n",
    "Ã‡Ã¼nki funksiyanÄ±n **tÃ¶rÉ™mÉ™lÉ™ri analitik ÅŸÉ™kildÉ™ tapÄ±lÄ±r vÉ™ hesablamaq asandÄ±r**. Gradient Descent É™sasÉ™n:  \n",
    "- TÃ¶rÉ™mÉ™lÉ™ri analitik hesablamaq Ã§É™tin vÉ™ ya mÃ¼mkÃ¼n olmadÄ±qda,  \n",
    "- Ã‡ox Ã¶lÃ§Ã¼lÃ¼ vÉ™ mÃ¼rÉ™kkÉ™b funksiyalar Ã¼Ã§Ã¼n optimal hÉ™ll tapmaq lazÄ±m gÉ™ldikdÉ™ istifadÉ™ olunur.  \n",
    "\n",
    "Bu halda, sadÉ™ **analitik tÃ¶rÉ™mÉ™** ilÉ™ lokal vÉ™ qlobal minimum/maksimumu tapa bilÉ™rik. ÆgÉ™r kritik nÃ¶qtÉ™lÉ™ri tapmaq lazÄ±mdÄ±rsa,  \n",
    "\n",
    "$$\n",
    "\\nabla f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right) = (0,0)\n",
    "$$\n",
    "\n",
    "bÉ™rabÉ™rliyini hÉ™ll edÉ™rÉ™k stasionar nÃ¶qtÉ™lÉ™ri tapa bilÉ™rik. ÆgÉ™r mÉ™qsÉ™diniz budursa, hesablaya bilÉ™rÉ™m. ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eac303c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b75b0899",
   "metadata": {},
   "source": [
    "## QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r bÉ™rabÉ™rdirsÉ™ Gradient Descent lazÄ±m deyil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27773210",
   "metadata": {},
   "source": [
    "**Gradient Descent**-É™ ehtiyac olub-olmamasÄ± É™sasÉ™n **funksiyanÄ±n tÃ¶rÉ™mÉ™lÉ™rinin hesablana bilmÉ™ vÉ™ziyyÉ™tindÉ™n vÉ™ optimallaÅŸdÄ±rma ehtiyacÄ±ndan** asÄ±lÄ±dÄ±r.  \n",
    "\n",
    "### 1ï¸âƒ£ **QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r bÉ™rabÉ™rdirsÉ™ Gradient Descent lazÄ±m deyil**  \n",
    "ÆgÉ™r qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r **analitik ÅŸÉ™kildÉ™** hesablanÄ±bsa vÉ™ aÃ§Ä±q formada ifadÉ™ edilÉ™ bilirsÉ™, **Gradient Descent lazÄ±m deyil**. Ã‡Ã¼nki optimal nÃ¶qtÉ™lÉ™ri **stasionar nÃ¶qtÉ™lÉ™ri tapÄ±b** analiz edÉ™rÉ™k mÃ¼É™yyÉ™n edÉ™ bilÉ™rik.  \n",
    "\n",
    "**Misal:**  \n",
    "ÆgÉ™r $ f(x, y) = x^3 y^2 + 5xy^3 $ Ã¼Ã§Ã¼n qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial^2 f}{\\partial x \\partial y} = 6x^2 y + 15y^2\n",
    "$$\n",
    "\n",
    "kimi **sadÉ™ vÉ™ hesablana bilÉ™n** ifadÉ™lÉ™rdirsÉ™, **Gradient Descent lazÄ±m deyil**, Ã§Ã¼nki biz onlarÄ± **tÉ™nlik kimi hÉ™ll edÉ™ bilÉ™rik**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ **Gradient Descent nÉ™ vaxt lazÄ±mdÄ±r?**  \n",
    "ÆgÉ™r qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r:  \n",
    "âœ… **Ã‡ox mÃ¼rÉ™kkÉ™bdirsÉ™** vÉ™ **É™l ilÉ™ hÉ™lli Ã§É™tindirsÉ™**,  \n",
    "âœ… **Qeyri-xÉ™tti diferensial tÉ™nliklÉ™r verirsÉ™** vÉ™ **algebraik olaraq hÉ™ll edilÉ™ bilmirsÉ™**,  \n",
    "âœ… **HÉ™lli Ã¼Ã§Ã¼n iterativ metodlar tÉ™lÉ™b olunursa**,  \n",
    "\n",
    "bu zaman **Gradient Descent vÉ™ ya digÉ™r optimallaÅŸdÄ±rma Ã¼sullarÄ±** lazÄ±m ola bilÉ™r.  \n",
    "\n",
    "**Misal:**  \n",
    "ÆgÉ™r $ f(x, y) $ elÉ™ bir funksiya olsaydÄ± ki, onun tÃ¶rÉ™mÉ™lÉ™ri Ã§ox mÃ¼rÉ™kkÉ™b bir sistem É™mÉ™lÉ™ gÉ™tirirdi:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = e^{x^2 + y^2} (2x + 3y^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial y} = \\ln(x^2 + y^2) (x - 4y^3)\n",
    "$$\n",
    "\n",
    "bu halda **É™l ilÉ™ hÉ™ll Ã§ox Ã§É™tin olardÄ±** vÉ™ **Gradient Descent vÉ™ ya Newtonâ€™s Method** kimi optimallaÅŸdÄ±rma Ã¼sullarÄ± tÉ™tbiq edilmÉ™li olardÄ±.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ **Yekun NÉ™ticÉ™**  \n",
    "ğŸ’¡ **ÆgÉ™r qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r aÃ§Ä±q ÅŸÉ™kildÉ™ ifadÉ™ edilirsÉ™ vÉ™ onlarÄ±n sÄ±fÄ±ra bÉ™rabÉ™r olduÄŸu nÃ¶qtÉ™lÉ™ri tapmaq mÃ¼mkÃ¼ndÃ¼rsÉ™, Gradient Descent lazÄ±m deyil.**  \n",
    "\n",
    "ğŸ” **ÆgÉ™r qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r Ã§ox mÃ¼rÉ™kkÉ™b vÉ™ ya qeyri-xÉ™tti bir sistem É™mÉ™lÉ™ gÉ™tirirsÉ™, Gradient Descent kimi iterativ metodlar istifadÉ™ edilÉ™ bilÉ™r.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a781d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21a001d1",
   "metadata": {},
   "source": [
    "## QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin sÄ±fÄ±r olmasÄ±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a40ec5c",
   "metadata": {},
   "source": [
    "**QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin sÄ±fÄ±r olmasÄ± analitik hÉ™llin olmamasÄ± demÉ™k deyil.**  \n",
    "\n",
    "ÆslindÉ™, qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin sÄ±fÄ±r olmasÄ± funksiyanÄ±n mÃ¼É™yyÉ™n istiqamÉ™tlÉ™rdÉ™ **dÉ™yiÅŸmÉ™mÉ™si** vÉ™ ya **dÃ¼z xÉ™tt boyunca sabit qalmasÄ±** kimi hallarÄ± ifadÉ™ edÉ™ bilÉ™r. Bu, hÉ™miÅŸÉ™ analitik hÉ™llin olmamasÄ± anlamÄ±na gÉ™lmir. Daha dÉ™qiq izah edÉ™k:  \n",
    "\n",
    "---  \n",
    "\n",
    "## **1ï¸âƒ£ QarÄ±ÅŸÄ±q TÃ¶rÉ™mÉ™ vÉ™ Onun SÄ±fÄ±r OlmasÄ±**  \n",
    "QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™ dedikdÉ™, bir funksiyanÄ±n iki dÉ™yiÅŸÉ™nÉ™ gÃ¶rÉ™ ikinci dÉ™rÉ™cÉ™li tÃ¶rÉ™mÉ™si nÉ™zÉ™rdÉ™ tutulur:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial x \\partial y}\n",
    "$$  \n",
    "\n",
    "ÆgÉ™r bu dÉ™yÉ™r **bÃ¼tÃ¼n nÃ¶qtÉ™lÉ™rdÉ™ sÄ±fÄ±rdÄ±rsa**, bu o demÉ™kdir ki, $ f(x, y) $-nin dÉ™yiÅŸmÉ™ sÃ¼rÉ™ti **x vÉ™ y dÉ™yiÅŸÉ™nlÉ™ri Ã¼zrÉ™ qarÅŸÄ±lÄ±qlÄ± tÉ™sir gÃ¶stÉ™rmir**.  \n",
    "\n",
    "Bu, iki mÃ¼mkÃ¼n vÉ™ziyyÉ™ti gÃ¶stÉ™rir:  \n",
    "- Funksiya **ayrÄ±lÄ±qda iki dÉ™yiÅŸÉ™nÉ™ gÃ¶rÉ™ xÉ™tti ola bilÉ™r**, mÉ™sÉ™lÉ™n:    \n",
    "  $$\n",
    "  f(x, y) = ax + by + c\n",
    "  $$  \n",
    "  Bu halda funksiya Ã§ox sadÉ™dir vÉ™ **analitik hÉ™ll tam mÃ¼mkÃ¼ndÃ¼r**.  \n",
    "- ÆgÉ™r funksiya daha mÃ¼rÉ™kkÉ™b bir quruluÅŸa malikdirsÉ™ vÉ™ **stasionar nÃ¶qtÉ™lÉ™rÉ™ malikdirsÉ™**, iterativ metodlar (mÉ™sÉ™lÉ™n, **Gradient Descent**) tÉ™lÉ™b oluna bilÉ™r.  \n",
    "\n",
    "---  \n",
    "\n",
    "## **2ï¸âƒ£ QarÄ±ÅŸÄ±q TÃ¶rÉ™mÉ™lÉ™rin SÄ±fÄ±r OlmasÄ± Analitik HÉ™llin OlmamasÄ± DemÉ™kdirmi?**  \n",
    "- **Xeyr, hÉ™r zaman yox!**    \n",
    "  QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin sÄ±fÄ±r olmasÄ± funksiyanÄ±n **ayrÄ±lÄ±qda $ x $ vÉ™ $ y $-yÉ™ gÃ¶rÉ™ asanlÄ±qla hÉ™ll edilÉ™ bilÉ™cÉ™yini** gÃ¶stÉ™rÉ™ bilÉ™r. BelÉ™ hallarda analitik hÉ™ll mÃ¶vcuddur vÉ™ iterativ metodlara ehtiyac olmur.  \n",
    "\n",
    "- **BÉ™zÉ™n iterativ metodlar tÉ™lÉ™b oluna bilÉ™r.**    \n",
    "  ÆgÉ™r funksiya **tÃ¶rÉ™mÉ™lÉ™ri sÄ±fÄ±r edÉ™n bir nÃ¶qtÉ™yÉ™ malikdirsÉ™ vÉ™ hÉ™min nÃ¶qtÉ™dÉ™ ikinci dÉ™rÉ™cÉ™li tÃ¶rÉ™mÉ™lÉ™r funksiyanÄ±n tipliliyini (minimum, maksimum, saddle point) tam mÃ¼É™yyÉ™n edÉ™ bilmirsÉ™**, Gradient Descent kimi **iterativ metodlar** istifadÉ™ edilÉ™ bilÉ™r.  \n",
    "\n",
    "---  \n",
    "\n",
    "## **3ï¸âƒ£ NÃ¼munÉ™lÉ™r ilÉ™ Ä°zah**  \n",
    "### **A. Analitik hÉ™llin mÃ¼mkÃ¼n olduÄŸu hallar**  \n",
    "ÆgÉ™r $ f(x, y) $ aÅŸaÄŸÄ±dakÄ± kimi sadÉ™dirsÉ™:  \n",
    "\n",
    "$$\n",
    "f(x, y) = 3x + 5y\n",
    "$$  \n",
    "\n",
    "Bu funksiyanÄ±n tÃ¶rÉ™mÉ™lÉ™ri belÉ™ olacaq:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = 3, \\quad \\frac{\\partial f}{\\partial y} = 5, \\quad \\frac{\\partial^2 f}{\\partial x \\partial y} = 0\n",
    "$$  \n",
    "\n",
    "Burada qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™ sÄ±fÄ±rdÄ±r, amma funksiya **sadÉ™ xÉ™ttidir vÉ™ analitik hÉ™ll aÃ§Ä±q ÅŸÉ™kildÉ™ mÃ¼mkÃ¼ndÃ¼r.** Burada heÃ§ bir optimallaÅŸdÄ±rma vÉ™ Gradient Descent tÉ™lÉ™b olunmur.  \n",
    "\n",
    "---  \n",
    "\n",
    "### **B. Gradient Descent TÉ™lÉ™b EdilÉ™n Hal**  \n",
    "ÆgÉ™r funksiyanÄ±z belÉ™dirsÉ™:  \n",
    "\n",
    "$$\n",
    "f(x, y) = x^2 y^2 + 5xy\n",
    "$$  \n",
    "\n",
    "Bunun qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™sini tapaq:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y} (2xy^2 + 5y) = 4xy + 5\n",
    "$$  \n",
    "\n",
    "Bu, mÃ¼É™yyÉ™n nÃ¶qtÉ™lÉ™rdÉ™ sÄ±fÄ±r ola bilÉ™r:  \n",
    "\n",
    "$$\n",
    "4xy + 5 = 0 \\quad \\Rightarrow \\quad xy = -\\frac{5}{4}\n",
    "$$  \n",
    "\n",
    "Bu halda, **stasionar nÃ¶qtÉ™lÉ™ri tapmaq Ã¼Ã§Ã¼n Gradient Descent kimi metodlara ehtiyac ola bilÉ™r**, Ã§Ã¼nki funksiyanÄ±n **qeyri-xÉ™tti olmasÄ± vÉ™ Ã§oxlu lokal ekstremumlara malik olmasÄ± mÃ¼mkÃ¼ndÃ¼r.**  \n",
    "\n",
    "---  \n",
    "\n",
    "## **4ï¸âƒ£ Son NÉ™ticÉ™**  \n",
    "- **QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin sÄ±fÄ±r olmasÄ± hÉ™miÅŸÉ™ analitik hÉ™llin olmamasÄ± demÉ™k deyil.**    \n",
    "- **ÆgÉ™r funksiya sadÉ™dirsÉ™, analitik hÉ™ll mÃ¼mkÃ¼ndÃ¼r.**    \n",
    "- **ÆgÉ™r funksiya mÃ¼rÉ™kkÉ™b vÉ™ qeyri-xÉ™ttidirsÉ™, stasionar nÃ¶qtÉ™lÉ™rin tapÄ±lmasÄ± Ã¼Ã§Ã¼n Gradient Descent kimi optimallaÅŸdÄ±rma metodlarÄ± lazÄ±m ola bilÉ™r.**    \n",
    "\n",
    "ÆgÉ™r konkret bir funksiya Ã¼zÉ™rindÉ™ araÅŸdÄ±rma aparmaq istÉ™yirsinizsÉ™, nÃ¼munÉ™ verin, birlikdÉ™ baxaq! ğŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be825ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "077eb842",
   "metadata": {},
   "source": [
    "## QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r sÄ±fÄ±rdÄ±rsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54370863",
   "metadata": {},
   "source": [
    " ÆgÉ™r **qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r sÄ±fÄ±rdÄ±rsa** (yÉ™ni funksiyanÄ±n gradienti sÄ±fÄ±ra bÉ™rabÉ™rdirsÉ™), o zaman **Gradient Descent** istifadÉ™ etmÉ™yin É™sas mÉ™qsÉ™di **optimal hÉ™llÉ™ Ã§atmaqdÄ±r**.  \n",
    "\n",
    "### 1ï¸âƒ£ **QarÄ±ÅŸÄ±q TÃ¶rÉ™mÉ™lÉ™r SÄ±fÄ±ra BÉ™rabÉ™rdirsÉ™:**  \n",
    "\n",
    "ÆgÉ™r $ \\nabla f(x, y) = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right) = (0, 0) $, yÉ™ni funksiyanÄ±n gradienti sÄ±fÄ±rdÄ±rsa, bu, **stasionar nÃ¶qtÉ™** olduÄŸunu bildirir. Ancaq bu nÃ¶qtÉ™ **minimum**, **maksimum** vÉ™ ya **saddlÉ™ÅŸmiÅŸ nÃ¶qtÉ™** (saddle point) ola bilÉ™r.  \n",
    "\n",
    "Bu halda, **Gradient Descent** istifadÉ™ etmÉ™k vacibdir, Ã§Ã¼nki bu metod **stasionar nÃ¶qtÉ™lÉ™ri tapmaq Ã¼Ã§Ã¼n iterativ olaraq minimuma yaxÄ±nlaÅŸmaq** Ã¼Ã§Ã¼n istifadÉ™ olunur. Gradient Descent-in É™sas mÉ™qsÉ™di **minimum nÃ¶qtÉ™sinÉ™ doÄŸru hÉ™rÉ™kÉ™t etmÉ™k**dir, xÃ¼susÉ™n dÉ™ analitik hÉ™ll tapmaq Ã§É™tin vÉ™ ya mÃ¼mkÃ¼n olmadÄ±qda.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ **Gradient Descent necÉ™ iÅŸlÉ™yir?**  \n",
    "Gradient Descent metodunun É™sas ideyasÄ± funksiyanÄ±n **gradientinÉ™ É™saslanaraq** (yÉ™ni tÃ¶rÉ™mÉ™lÉ™rinÉ™ É™saslanaraq) funksiyanÄ± minimuma doÄŸru optimallaÅŸdÄ±rmaqdÄ±r. HÉ™r iterasiyada, cari nÃ¶qtÉ™dÉ™n funksiyanÄ±n gradientinÉ™ É™saslanaraq nÃ¶vbÉ™ti nÃ¶qtÉ™ hesablanÄ±r.  \n",
    "\n",
    "**Gradient Descent formulasÄ±:**  \n",
    "\n",
    "$$\n",
    "\\theta_{new} = \\theta_{old} - \\alpha \\nabla f(\\theta)\n",
    "$$  \n",
    "\n",
    "- $ \\theta $ - parametrlÉ™r (burada $ x $ vÉ™ $ y $ ola bilÉ™r)  \n",
    "- $ \\alpha $ - Ã¶yrÉ™nmÉ™ sÃ¼rÉ™ti (learning rate)  \n",
    "- $ \\nabla f(\\theta) $ - funksiyanÄ±n gradienti (tÃ¶rÉ™mÉ™lÉ™ri)  \n",
    "\n",
    "Bu tÉ™nlikdÉ™ $ \\alpha $ Ã§ox bÃ¶yÃ¼k vÉ™ ya Ã§ox kiÃ§ik seÃ§ildikdÉ™, optimallaÅŸdÄ±rma prosesi ya Ã§ox sÃ¼rÉ™tli, ya da Ã§ox yavaÅŸ ola bilÉ™r. ÆgÉ™r $ \\alpha $ Ã§ox bÃ¶yÃ¼kdÃ¼rsÉ™, adÉ™tÉ™n hÉ™llin optimallaÅŸmasÄ± tÉ™krarlanmaz, Ã§Ã¼nki hÉ™r addÄ±mda Ã§ox bÃ¶yÃ¼k dÉ™yiÅŸikliklÉ™r baÅŸ verÉ™r.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ **Gradient Descent NÉ™ Vaxt TÉ™tbiq Edilir?**  \n",
    "Gradient Descent aÅŸaÄŸÄ±dakÄ± hallarda istifadÉ™ olunur:  \n",
    "\n",
    "- **FunksiyanÄ±n analitik olaraq hÉ™lli mÃ¼mkÃ¼n deyil** vÉ™ ya Ã§ox mÃ¼rÉ™kkÉ™bdir (funksiyanÄ±n tÃ¶rÉ™mÉ™si vÉ™ ya sÄ±fÄ±ra bÉ™rabÉ™r olduÄŸu nÃ¶qtÉ™lÉ™r É™l ilÉ™ tapÄ±la bilmÉ™z).  \n",
    "- **Funksiya Ã§ox bÃ¶yÃ¼k Ã¶lÃ§Ã¼lÃ¼ vÉ™ ya qeyri-xÉ™tti** olur (mÉ™sÉ™lÉ™n, Ã§ox sayda dÉ™yiÅŸÉ™ni olan vÉ™ ya Ã§ox mÃ¼rÉ™kkÉ™b formullarÄ± olan).  \n",
    "- **Ã‡oxsaylÄ± stasionar nÃ¶qtÉ™lÉ™r mÃ¶vcuddur** vÉ™ optimal hÉ™llin tapÄ±lmasÄ± Ã§É™tindir (minimallaÅŸdÄ±rma vÉ™ ya optimallaÅŸdÄ±rma tapÅŸÄ±rÄ±qlarÄ±).  \n",
    "\n",
    "---\n",
    "\n",
    "### 4ï¸âƒ£ **NÉ™ticÉ™**  \n",
    "ÆgÉ™r qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r sÄ±fÄ±rdÄ±rsa vÉ™ funksiyanÄ±n analitik hÉ™lli Ã§É™tindirsÉ™, **Gradient Descent** istifadÉ™ etmÉ™k Ã§ox faydalÄ±dÄ±r. Bu metod funksiyanÄ± **iterativ ÅŸÉ™kildÉ™ optimallaÅŸdÄ±rmaÄŸa** kÃ¶mÉ™k edir vÉ™ yerli minimumu tapmaq Ã¼Ã§Ã¼n istifadÉ™ edilir.  \n",
    "\n",
    "EhtiyacÄ±nÄ±z varsa, **Gradient Descent**-in Python-da tÉ™tbiqi Ã¼Ã§Ã¼n nÃ¼munÉ™ verÉ™ bilÉ™rÉ™m! ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee768c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6c93ac3",
   "metadata": {},
   "source": [
    "## ÃœmumilÉ™ÅŸdrimÉ™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d484e",
   "metadata": {},
   "source": [
    "### 1ï¸âƒ£ **QarÄ±ÅŸÄ±q TÃ¶rÉ™mÉ™lÉ™rin BÉ™rabÉ™rliyi:**  \n",
    "ÆgÉ™r **qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r bÉ™rabÉ™rdirsÉ™**, yÉ™ni  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial^2 f}{\\partial x \\partial y},\n",
    "$$  \n",
    "\n",
    "bu, **Clairaut-un teoremi**nÉ™ gÃ¶rÉ™ **funksiyanÄ±n hÉ™m $ x $-É™, hÉ™m dÉ™ $ y $-yÉ™ gÃ¶rÉ™ tÃ¼revlerinin mÃ¼badilÉ™si mÃ¼mkÃ¼n olduÄŸunu** bildirir. Bu halda **funksiya hamar (differensiyalanabilÉ™n)** sayÄ±lÄ±r vÉ™ **analitik hÉ™ll** mÃ¼mkÃ¼ndÃ¼r. YÉ™ni, bu halda funksiyanÄ±n **tÃ¶rÉ™mÉ™lÉ™ri mÃ¶vcuddur vÉ™ dÃ¼zgÃ¼n bir ÅŸÉ™kildÉ™ hesablanÄ±r**.  \n",
    "\n",
    "**QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin bÉ™rabÉ™r olmasÄ±** funksiyanÄ±n analitik hÉ™lli olduÄŸu anlamÄ±na gÉ™lir, Ã§Ã¼nki bu, funksiyanÄ±n **davamlÄ± vÉ™ diferensiyalanabilÉ™n olduÄŸunu** gÃ¶stÉ™rir.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ **QarÄ±ÅŸÄ±q TÃ¶rÉ™mÉ™lÉ™rin SÄ±fÄ±r OlmasÄ±:**  \n",
    "ÆgÉ™r **qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r sÄ±fÄ±rdÄ±rsa** (yÉ™ni, bir vÉ™ ya daha Ã§ox tÃ¶rÉ™mÉ™ sÄ±fÄ±ra bÉ™rabÉ™rdirsÉ™), bu, funksiyanÄ±n **stasionar nÃ¶qtÉ™lÉ™rÉ™ sahib olduÄŸunu** bildirÉ™ bilÉ™r. Lakin **qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin sÄ±fÄ±r olmasÄ±** funksiyanÄ±n **hamar olmamasÄ±** demÉ™k deyil. Funksiya **davamlÄ±** vÉ™ **diferensiyalanabilÉ™n** ola bilÉ™r, amma bu, sadÉ™cÉ™ onun **minimum vÉ™ ya maksimum nÃ¶qtÉ™ olduÄŸunu** gÃ¶stÉ™rir.  \n",
    "\n",
    "**QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin sÄ±fÄ±r olmasÄ±**, optimallaÅŸdÄ±rma prosesindÉ™ **Gradient Descent** kimi metodlara ehtiyac olduÄŸunu gÃ¶stÉ™rir. Bu, funksiyanÄ±n **minimum, maksimum vÉ™ ya saddle point** (saddlÉ™ÅŸmiÅŸ nÃ¶qtÉ™) tapma prosesi ilÉ™ baÄŸlÄ± ola bilÉ™r.  \n",
    "\n",
    "**QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin sÄ±fÄ±r olmasÄ±** funksiyanÄ±n **differensiyalanabilÉ™n olduÄŸunu**, amma **qeyri-xÉ™tti vÉ™ ya qeyri-analitik olduÄŸunu** gÃ¶stÉ™rmÉ™z. Bu vÉ™ziyyÉ™tdÉ™ dÉ™ analitik hÉ™ll mÃ¼mkÃ¼ndÃ¼r, amma o hÉ™ll **iterativ optimallaÅŸdÄ±rma Ã¼sullarÄ± ilÉ™** tapÄ±la bilÉ™r.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ **NÉ™ticÉ™:**  \n",
    "- **QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin bÉ™rabÉ™r olmasÄ± (Clairaut-un teoremi)**: Funksiya **hamardÄ±r** vÉ™ analitik hÉ™ll mÃ¶vcuddur.  \n",
    "- **QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin sÄ±fÄ±r olmasÄ±**: Funksiya **differensiyalanabilÉ™ndir**, lakin bununla yanaÅŸÄ±, funksiyanÄ±n optimallaÅŸdÄ±rÄ±lmasÄ± Ã¼Ã§Ã¼n **iterativ metodlar** (mÉ™sÉ™lÉ™n, Gradient Descent) lazÄ±m ola bilÉ™r. Bu, funksiyanÄ±n minimum, maksimum vÉ™ ya saddle point tapmaq mÉ™qsÉ™dilÉ™ edilÉ™ bilÉ™r.  \n",
    "\n",
    "BelÉ™liklÉ™, qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin sÄ±fÄ±r olmasÄ± funksiyanÄ±n **hamar olmamasÄ±** deyil, sadÉ™cÉ™ **stasionar nÃ¶qtÉ™lÉ™rin** olduÄŸunu vÉ™ optimallaÅŸdÄ±rma Ã¼sullarÄ±nÄ±n lazÄ±m ola bilÉ™cÉ™yini gÃ¶stÉ™rir.  \n",
    "\n",
    "ÆgÉ™r baÅŸqa suallarÄ±nÄ±z varsa, mÉ™nÉ™ bildirin! ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b3f49e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
